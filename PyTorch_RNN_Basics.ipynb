{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RNN 기초 학습, PyTorch로 배우기 ('24.07.31 ~ '24.08.01)\n",
        "\n",
        "> References : <br>\n",
        "https://wikidocs.net/60760, https://wikidocs.net/64515"
      ],
      "metadata": {
        "id": "JB25hYwKs0dm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recurrent Neural Network\n",
        "\n",
        "> 대표적인 시퀀스 모델 : 입출력을 시퀀스 단위로 처리하는 모델. 문장은 단어 시퀀스이고, 이들을 처리하기 위해 고안된 모델이다. RNN은 딥러닝에 있어 가장 기본적인 시퀀스 모델임.\n",
        "\n",
        "- FeedForward NN : 은닉층에서 활성화 함수를 통과한 값이 오직 출력층 방향으로만(한 방향) 향하는 신경망.\n",
        "\n",
        "- <-> RNN : 은닉층의 노드에서 활성화 함수를 통과한 결과값을 출력층 방향으로도 보내면서, 다시 ***은닉층 노드의 다음 계산의 입력으로 동시에 보내는*** 특징을 갖는다.\n",
        "\n",
        "- Memory(RNN) cell : 은닉층에서 활성화 함수를 통해 결과를 내보내는 역할을 하는 노드이며, 이전의 값을 기억하려 하는 메모리 역할을 수행한다. 각각의 시점에서, 바로 이전 시점에서의 은닉층의 메모리 셀에서 나온 output 값을 자신의 입력으로 사용하는 재귀적 활동을 한다.\n",
        "\n",
        "- Hidden state : 메모리 셀이 출력층 방향 <=> 다음 시점 (t + 1) 셀로 보내는 값을 말한다.\n",
        "\n",
        "<br>\n",
        "\n",
        "### 모델의 형태 3가지\n",
        "- one-to-many : 이미지 캡셔닝 작업 등에 사용될 수 있다. (이미지 한 개 입력 -> 이미지의 제목 문장(단어들) 출력)\n",
        "- many-to-one : 스팸 메일 분류 작업 등에 사용될 수 있다. (스팸 의심 메일 입력 -> 스팸 메일 여부(T/F) 출력)\n",
        "- many-to-many : 챗봇, 번역기 등. 입력 문장으로부터 대답 문장을 출력하는 작업에 사용될 수 있다.\n",
        "\n"
      ],
      "metadata": {
        "id": "ztoYyp5AtRhV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 파이썬으로 RNN 셀 한 층을 구현하기(low-level)\n",
        "\n",
        "- 실제 PyTorch에서는 (batch_size, timesteps, input_size) 크기의 3D 텐서를 입력으로 받음."
      ],
      "metadata": {
        "id": "txgFWZ-A0XRe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdvCcRO9sxzV",
        "outputId": "c7846388-547e-434f-b708-a724bdc7efd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "8\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "timesteps = 5 # 시점의 수. NLP에서는 보통 문장의 길이가 된다.\n",
        "input_size = 4 # 입력의 차원. NLP에서는 보통 단어 벡터의 차원이 된다. (d)\n",
        "hidden_size = 8 # 은닉 상태의 크기. 메모리 셀의 용량이다. (D_h)\n",
        "\n",
        "inputs = np.random.random((timesteps, input_size)) # 입력에 해당하는 2D 텐서(실제로는 batch size까지 3D로 입력받음)\n",
        "\n",
        "hidden_state_t = np.zeros((hidden_size, )) # 초기 은닉 상태는 0(벡터)으로 초기화. (8, ) 의 크기를 가짐\n",
        "\n",
        "print(hidden_state_t)\n",
        "print(hidden_state_t.size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (8, 4) 크기의 랜덤 값을 갖는 2D 텐서 생성. 입력에 대한 Weight 행렬\n",
        "W_x = np.random.random((hidden_size, input_size))\n",
        "\n",
        "# (8, 8) 크기의 랜덤 값을 갖는 2D 텐서 생성. hidden state에 대한 Weight 행렬(정사각행렬)\n",
        "W_h = np.random.random((hidden_size, hidden_size))\n",
        "\n",
        "# (8, ) 크기의 랜덤 값을 갖는 1D 텐서 생성. bias(편향)\n",
        "b = np.random.random((hidden_size, ))\n",
        "\n",
        "print(np.shape(W_x)) ; print(np.shape(W_h)) ; print(np.shape(b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtKufVNLl1KH",
        "outputId": "5cce0a11-7f52-4d6b-e8ef-d9ad0c123f64"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 4)\n",
            "(8, 8)\n",
            "(8,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_hidden_states = []\n",
        "\n",
        "# 메모리 셀 동작\n",
        "for input_t in inputs: # 각 시점에 따라서 입력값이 입력됨\n",
        "    output_t = np.tanh(np.dot(W_x, input_t) + np.dot(W_h, hidden_state_t) + b)\n",
        "\n",
        "    total_hidden_states.append(list(output_t)) # 각 시점의 은닉 상태의 값을 계속해서 축적\n",
        "\n",
        "    print(np.shape(total_hidden_states))\n",
        "\n",
        "    hidden_state_t = output_t\n",
        "\n",
        "total_hidden_states = np.stack(total_hidden_states, axis = 0)\n",
        "print(total_hidden_states) # (timesteps, output_dim)의 크기를 갖는 메모리 셀의 2D 텐서들이 출력됨"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuV-tMwFnEH2",
        "outputId": "962d5eae-f09d-4cb9-9ddf-5dc030a4fc1d"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 8)\n",
            "(2, 8)\n",
            "(3, 8)\n",
            "(4, 8)\n",
            "(5, 8)\n",
            "[[0.94013076 0.95204053 0.99111907 0.99566879 0.97696259 0.97252909\n",
            "  0.96043119 0.98296719]\n",
            " [0.99975433 0.99995622 0.99998792 0.99995452 0.99996576 0.99999221\n",
            "  0.9998896  0.99986982]\n",
            " [0.99972658 0.99988476 0.99998849 0.99991301 0.99996335 0.99999542\n",
            "  0.99985736 0.99970847]\n",
            " [0.99981788 0.99996545 0.99999532 0.99998121 0.99998951 0.99999711\n",
            "  0.99993062 0.99995727]\n",
            " [0.99982683 0.9999509  0.99997696 0.99990008 0.99996616 0.99999029\n",
            "  0.99989072 0.99976223]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch의 nn.RNN()\n",
        "\n",
        "- hidden state의 크기인 hidden_size 는 대표적인 RNN의 Hyperparameter로, 사용자가 지정한다.\n",
        "\n",
        "- timesteps 의미: 시점의 수\n",
        "    - NLP에서의 의미: 문장의 길이, 즉 문장 내 단어의 개수\n",
        "    - 예시: 만약 문장이 10개의 단어로 구성되어 있다면, timesteps는 10이 된다.\n",
        "\n",
        "- input_size 의미: 입력 벡터의 차원\n",
        "    - NLP에서의 의미: 각 단어 벡터의 차원\n",
        "    - 예시: One-Hot Encoding 사용 시 어휘의 크기가 5인 경우, 각 단어는 5차원의 원-핫 벡터로 표현되므로 input_size는 5가 된다."
      ],
      "metadata": {
        "id": "Vl1an-8Lo0QV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "batch_size = 1\n",
        "timesteps = 10\n",
        "input_size = 5 # 입력의 크기\n",
        "hidden_size = 8 # 은닉 상태의 크기"
      ],
      "metadata": {
        "id": "_Z-Qc8IKod1j"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력 텐서 정의 : (batch_size, timesteps, input_size)\n",
        "inputs = torch.Tensor(batch_size, timesteps, input_size) # 배치 크기는 1, 10번의 시점 동안 5차원의 입력 벡터가 들어가도록 텐서 정의"
      ],
      "metadata": {
        "id": "u_UmzlOjpVde"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN cell 정의 : nn.RNN() 메소드 활용\n",
        "cell = nn.RNN(input_size, hidden_size, batch_first = True)\n",
        "# batch_first = True : 입력 텐서의 첫 번째 차원이 batch size임을 알려주는 파라미터"
      ],
      "metadata": {
        "id": "ZXhlIfi3qIdj"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력 텐서를 RNN 셀에 입력하여, 출력을 확인한다.(high-level)\n",
        "outputs, status = cell(inputs)"
      ],
      "metadata": {
        "id": "3587JS93qUt6"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- RNN 셀은 두 개의 입력을 리턴한다. 첫 번째 리턴값은 모든 시점의 hidden state들이며, \"두 번째 리턴값은 마지막 시점\"의 hidden state이다."
      ],
      "metadata": {
        "id": "B02ypq-Iq23P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.shape) # 10번의 시점 동안, 8차원의 hidden state가 출력되었다는 의미. 10이라는 숫자는 차원과는 연관 없음.\n",
        "print(status.shape) # 최종 time-step인 10번째의 hidden state."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qmOKJxjq9iU",
        "outputId": "6bf32ed5-83dc-4341-d837-a897ae768b96"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10, 8])\n",
            "torch.Size([1, 1, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deep RNN(은닉층 여러 개)\n",
        "\n",
        "- PyTorch 구현 시, nn.RNN()의 인자인 num_layers 옵션에 값을 부여하여 층을 쌓는 방식으로 구현한다.\n",
        "\n",
        "> nn.RNN(input_size, hidden_size, num_layers, batch_first)"
      ],
      "metadata": {
        "id": "wM8pOguhrwiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.Tensor(1, 10, 5)\n",
        "\n",
        "cell = nn.RNN(input_size = 5, hidden_size = 8, num_layers = 2, batch_first = True)\n",
        "\n",
        "outputs, status = cell(inputs)\n",
        "\n",
        "# 모든 time-step의 hidden_state. 첫 번째 리턴값의 크기는 layer가 한 개인 RNN과 동일. \"마지막 층의 모든 시점\"에서의 상태.\n",
        "print(outputs.shape)\n",
        "\n",
        "# 두 번째 리턴값의 크기는 차이가 있음. [층의 개수, 배치 사이즈, 은닉 상태의 크기]에 해당함.\n",
        "print(status.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxIaBE4wreBi",
        "outputId": "f46c0d1f-c3d2-4efa-d1ca-877f9aed03d8"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10, 8])\n",
            "torch.Size([2, 1, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 양방향 순환 신경망(Bidirectional RNN)\n",
        "\n",
        "- 시점 t에서의 출력값을 예측할 때, 이전 시점의 데이터(기존) 뿐만 아니라 ***이후 데이터로도 예측할 수 있다***는 아이디어에 기반한다.\n",
        "\n",
        "- 양방향 RNN은 하나의 출력값을 예측하기 위해 두 개의 메모리 셀을 사용한다.\n",
        "    - 첫 번째 메모리 셀은 기존과 동일하게 앞 시점의 은닉 상태를 전달받아 현재의 은닉 상태를 계산한다.\n",
        "    - 두 번째 메모리 셀은 앞 시점의 은닉 상태가 아닌, 뒤 시점의 은닉 상태(Backward states)를 전달받아 현재의 은닉 상태를 계산한다."
      ],
      "metadata": {
        "id": "7__qtrGjtz70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.Tensor(1, 10, 5)\n",
        "\n",
        "cell = nn.RNN(input_size = 5, hidden_size = 8,\n",
        "              num_layers = 2, batch_first = True,\n",
        "              bidirectional = True) # Hidden Layer가 2개이고, 양방향 순환 신경망인 경우\n",
        "\n",
        "outputs, status = cell(inputs)\n",
        "\n",
        "print(outputs.shape)\n",
        "print(status.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRPYhnI8tOET",
        "outputId": "8e7799dd-84ee-4201-c103-b153bd936491"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10, 16])\n",
            "torch.Size([4, 1, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**해석**\n",
        "\n",
        "1. outputs.shape : (배치 크기, 시퀀스 길이, 은닉 상태의 크기 x 2) -> 첫 번째 리턴값의 크기는 단방향 RNN보다 은닉 상태의 크기가 두 배인 16이 됨. \"양방향 은닉 상태 값들이 연결됨(concatenate).\"\n",
        "\n",
        "2. status.shape : (은닉층 개수 x 2, 배치 크기, 은닉 상태의 크기) -> {정방향 기준 마지막 시점 + 역방향 기준 첫 번째 시점}이므로, 해당하는 시점(동일)의 출력값을 층의 개수만큼 쌓아 올린 결과값이 된다."
      ],
      "metadata": {
        "id": "NgjKSKvgxPVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM(Long Short-Term Memory)\n",
        "\n",
        "- Vanila(=Simple) RNN의 한계를 극복하기 위한 다양한 RNN의 변형 중 하나이다.\n",
        "\n",
        "### 바닐라 RNN의 한계 : 장기 의존성 문제\n",
        "-> Problem of Long-Term Dependencies. 비교적 짧은 시퀀스에 대해서만 효과를 보인다. 바닐라 RNN의 시점(timesteps)이 길어질수록, 앞의 정보가 뒤로 충분히 전달되지 못하는 현상이 발생한다. \"앞의 정보가 문장에서 중요\"한 역할을 하는 경우 단어 예측을 엉뚱하게 수행할 수 있다.\n",
        "\n",
        "### LSTM 내부구조\n",
        "-> LSTM은 은닉층의 메모리 셀에 <입력 게이트, 망각(삭제) 게이트, 출력 게이트> 를 추가하여, 불필요한 기억들을 지우고 기억해야 할 것들을 선별한다. + cell state(셀 상태)라는 값을 추가하여 은닉 사태를 계산하는 식이 바닐라 RNN보다 복잡해지지만, 긴 시퀀스의 입력 처리 시 개선된 성능을 보인다.\n",
        "\n",
        "- 셀 상태 : 이전 시점의 셀 상태가 다음 시점의 셀 상태를 구하기 위한 입력으로서 사용된다.\n",
        "\n",
        "- 입력 게이트, 망각 게이트, 출력 게이트에는 공통적으로 sigmoid가 존재하여, 0 ~ 1의 값으로 (망각/기억) 게이트 출력을 조절한다.\n",
        "\n",
        "- 망각 게이트는 이전 시점의 입력을 얼마나 반영할지 결정하고, 입력 게이트는 현재 시점의 입력을 얼마나 반영할지를 결정하는 역할을 수행한다.\n",
        "\n",
        "### PyTorch의 nn.LSTM()\n",
        "-> nn.LSTM(input_dim, hidden_size, batch_first) # RNN 셀과 동일한 파라미터"
      ],
      "metadata": {
        "id": "F6-p8y21xshI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM 사용 예시\n",
        "inputs = torch.Tensor(1, 10, 5)\n",
        "\n",
        "cell = nn.LSTM(input_size = 5, hidden_size = 8,\n",
        "              num_layers = 2, batch_first = True,\n",
        "              bidirectional = True) # LSTM Layer가 2개이고, 양방향 LSTM인 경우\n",
        "\n",
        "outputs, (hidden_state, cell_state) = cell(inputs)\n",
        "\n",
        "print(outputs.shape)\n",
        "print(hidden_state.shape)\n",
        "print(cell_state.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKoSORKIwr3m",
        "outputId": "07f4e06b-5e81-426a-ba13-ef7c6160c5f2"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10, 16])\n",
            "torch.Size([4, 1, 8])\n",
            "torch.Size([4, 1, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**해석**\n",
        "\n",
        "1. outputs : 각 시점에 대한 모든 레이어의 출력이다. 양방향 LSTM이기 떄문에, 각 시점에서 순방향과 역방향의 출력을 합친 8 + 8 = 16차원의 벡터가 된다.\n",
        "\n",
        "2. hidden_state : (num_layer x 2(양방향) : 4, batch_size : 1, hidden_size : 8)\n",
        "\n",
        "3. cell_state : hidden_state와 상동\n",
        "\n",
        "차별점 : LSTM이므로 두 번째 인자로 Vanila RNN와 달리 <br> status -> (hidden_state, cell_state)의 tuple을 리턴받음."
      ],
      "metadata": {
        "id": "Z0lsUiQtwxPi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRU(Gated Recurrent Unit, 게이트 순환 유닛)\n",
        "-> LSTM의 여전한 장기 의존성 문제에 대한 해결책을 유지하면서, 은닉 상태를 업데이트하는 계산을 간소화하였다.\n",
        "\n",
        "- GRU와 LSTM 중 성능 측면에서 더 나은 것을 단정지을 수는 없다. LSTM으로 최적의 하이퍼파라미터를 찾아낸 상황이라면 굳이 바꿔 사용할 필요는 없다.\n",
        "\n",
        "- 경험적으로 데이터 양이 소량일 때는 매개변수의 양이 적은 GRU가 조금 더 낫고, 데이터 양이 많을 경우 LSTM이 더 낫다고 본다고 한다.\n",
        "\n",
        "### PyTorch의 nn.GRU()\n",
        "-> nn.GRU(input_dim, hidden_size, batch_first) # RNN 셀과 동일한 파라미터"
      ],
      "metadata": {
        "id": "gx6pNp_4yNvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM 사용 예시\n",
        "inputs = torch.Tensor(1, 10, 5)\n",
        "\n",
        "cell = nn.GRU(input_size = 5, hidden_size = 8,\n",
        "              num_layers = 2, batch_first = True,\n",
        "              bidirectional = True) # GRU Layer가 2개이고, 양방향 GRU인 경우\n",
        "\n",
        "outputs, cell_state = cell(inputs) # GRU는 hidden state만 리턴한다. (LSTM 간소화)\n",
        "\n",
        "print(outputs.shape)\n",
        "print(cell_state.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZeGs7C8wk9_",
        "outputId": "03037367-3dad-4998-c47d-6d6d40e80657"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10, 16])\n",
            "torch.Size([4, 1, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 문자 단위 RNN(Char RNN)\n",
        "-> RNN의 입출력의 단위가 단어 레벨이 아니라, 문자 레벨로 하여 RNN을 구현한 것을 말한다.\n",
        "\n",
        "---\n",
        "\n",
        "## I. 문자 시퀀스 apple 입력 -> pple!을 출력하는 RNN 구현\n",
        "\n",
        "### 1. 훈련 데이터 전처리"
      ],
      "metadata": {
        "id": "hK2x82Gw2P_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "bBa_VYNM1ahg"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_str = 'apple'\n",
        "label_str = 'pple!'\n",
        "\n",
        "# set으로 중복되지 않게 string을 char로 분리 -> list에 문자들 담기 -> 사전순 오름차순 정렬\n",
        "char_vocab = sorted(list(set(input_str + label_str)))"
      ],
      "metadata": {
        "id": "Bl7gmrD-3bxj"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터 정의 : 입력이 One-Hot Vector이므로 입력의 크기는 문자 집합의 크기와 동일해야 한다.\n",
        "vocab_size = len(char_vocab)\n",
        "input_size = vocab_size\n",
        "\n",
        "hidden_size = 5\n",
        "output_size = 5\n",
        "\n",
        "learning_rate = 0.1"
      ],
      "metadata": {
        "id": "dy5c-G9137yP"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문자 집합에 고유한 정수를 부여 : 레이블링\n",
        "char_to_index = dict((c, i) for i, c in enumerate(char_vocab)) # 리스트 원소 -> 딕셔너리화\n",
        "print(char_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qv8tQOXe4QFM",
        "outputId": "055dfbd4-0118-4a14-97aa-32a64eec8e46"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측 결과를 다시 문자 시퀀스로 보기 위해서, 레이블된 정수로부터 문자를 얻을 수 있게 함\n",
        "index_to_char = {}\n",
        "\n",
        "for key, value in char_to_index.items():\n",
        "    index_to_char[value] = key # key, value 바꿔주면 끝\n",
        "print(index_to_char)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avE7k8O44x4Y",
        "outputId": "f3c2a39f-8b8c-4bc1-b791-fa47392b30bb"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_data = [char_to_index[c] for c in input_str] # list\n",
        "y_data = [char_to_index[c] for c in label_str] # list\n",
        "print(x_data) # a, p, p, l, e\n",
        "print(y_data) # p, p, l, e, !"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZC7QFh325EhO",
        "outputId": "74c44506-64e3-4a2d-9fad-855fa9d1736a"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 4, 4, 3, 2]\n",
            "[4, 4, 3, 2, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- PyTorch의 nn.RNN()은 3차원 텐서를 입력받는다. 배치 차원 추가가 필요함."
      ],
      "metadata": {
        "id": "zKOewP5-5958"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_data = torch.tensor(x_data).unsqueeze(0) # x_data = [x_data]\n",
        "y_data = torch.tensor(y_data).unsqueeze(0) # y_data = [y_data]\n",
        "print(x_data)\n",
        "print(y_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cZ3uQDc50Ji",
        "outputId": "55bf952f-3820-4f08-fa71-a582304f9c0d"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 4, 4, 3, 2]])\n",
            "tensor([[4, 4, 3, 2, 0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- x_data : 1(a), 4(p), 4(p), 3(l), 2(e)\n",
        "- vocab_size : 5(!, a, e, l, p 5개)\n",
        "- np.eye : NumPy 단위행렬 생성 메소드"
      ],
      "metadata": {
        "id": "LgKrYPyT7O4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력 시퀀스의 각 문자들을 One-Hot Vector화\n",
        "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n",
        "print(x_one_hot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6A2pg2e6qnb",
        "outputId": "1df1a509-1955-493d-a153-f32cac124e23"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[0., 1., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 1., 0.],\n",
            "       [0., 0., 1., 0., 0.]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력, 레이블 데이터 텐서화\n",
        "x_one_hot = np.array(x_one_hot)\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)\n",
        "\n",
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
        "print('레이블의 크기 : {}'.format(Y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9mB3cpO7okC",
        "outputId": "03f32fc2-363e-4da1-d514-d857a6d14f11"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터의 크기 : torch.Size([1, 5, 5])\n",
            "레이블의 크기 : torch.Size([1, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 모델 구현하기"
      ],
      "metadata": {
        "id": "ViuhgpJW8WY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(Net, self).__init__()\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first = True) # RNN 셀 구현\n",
        "        self.fc = nn.Linear(hidden_size, output_size, bias = True) # 출력층(Fully Connected)\n",
        "\n",
        "    def forward(self, x): # 구현한 RNN 셀과 출력층 연결\n",
        "        x, status = self.rnn(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "XsJ4WopJ7zKK"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net(input_size, hidden_size, output_size)\n",
        "\n",
        "outputs = net(X)\n",
        "print(outputs.shape) # 3D 텐서, [배치 차원, \"시점(timesteps)\", 출력의 크기]\n",
        "print(Y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlDSKQfT9M5a",
        "outputId": "dc7bbebc-dc22-496a-8f01-244ba955af77"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5, 5])\n",
            "torch.Size([1, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2D 텐서로 변환, view를 사용하여 배치 차원과 시점 차원을 하나로 만든다. 정확도(accuracy) 측정 시 사용됨.\n",
        "print(outputs.view(-1, input_size).shape)\n",
        "\n",
        "print(Y.view(-1).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdrtEf9-9UrI",
        "outputId": "10da516c-c0d9-42cf-cb99-ab0b0134e6a2"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 5])\n",
            "torch.Size([5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)"
      ],
      "metadata": {
        "id": "x4LwioJii5eM"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCH = 50\n",
        "\n",
        "for epoch in range(EPOCH + 1):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = net(X)\n",
        "\n",
        "    # (prediction, 정답 Y) : view를 하는 이유는 Batch 차원 제거를 위함\n",
        "    loss = criterion(outputs.view(-1, input_size), Y.view(-1))\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step() # 파라미터 업데이트\n",
        "\n",
        "    # - - - 모델 예측 확인 코드 - - -\n",
        "    result = outputs.data.numpy().argmax(axis = 2) # 최종 예측값인 각 timestep 별 5차원 벡터에 대해서 가장 높은 값의 인덱스를 선택\n",
        "    result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n",
        "    if epoch % 5 == 0:\n",
        "        print(\"epoch\", epoch, \"/ loss :\", loss.item(), \"/ prediction :\", result,\n",
        "              \"/ true Y :\", y_data, \"/ prediction str :\", result_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkl7g27pjOFZ",
        "outputId": "a11f48dc-5ddd-48cd-9c3b-68a0bb9e1150"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 / loss : 1.8081724643707275 / prediction : [[0 0 0 0 0]] / true Y : tensor([[4, 4, 3, 2, 0]]) / prediction str : !!!!!\n",
            "epoch 5 / loss : 0.8989709615707397 / prediction : [[4 4 4 4 0]] / true Y : tensor([[4, 4, 3, 2, 0]]) / prediction str : pppp!\n",
            "epoch 10 / loss : 0.3196743130683899 / prediction : [[4 4 3 2 0]] / true Y : tensor([[4, 4, 3, 2, 0]]) / prediction str : pple!\n",
            "epoch 15 / loss : 0.06679805368185043 / prediction : [[4 4 3 2 0]] / true Y : tensor([[4, 4, 3, 2, 0]]) / prediction str : pple!\n",
            "epoch 20 / loss : 0.01642611250281334 / prediction : [[4 4 3 2 0]] / true Y : tensor([[4, 4, 3, 2, 0]]) / prediction str : pple!\n",
            "epoch 25 / loss : 0.0056962063536047935 / prediction : [[4 4 3 2 0]] / true Y : tensor([[4, 4, 3, 2, 0]]) / prediction str : pple!\n",
            "epoch 30 / loss : 0.0028663554694503546 / prediction : [[4 4 3 2 0]] / true Y : tensor([[4, 4, 3, 2, 0]]) / prediction str : pple!\n",
            "epoch 35 / loss : 0.0017348633846268058 / prediction : [[4 4 3 2 0]] / true Y : tensor([[4, 4, 3, 2, 0]]) / prediction str : pple!\n",
            "epoch 40 / loss : 0.0012577483430504799 / prediction : [[4 4 3 2 0]] / true Y : tensor([[4, 4, 3, 2, 0]]) / prediction str : pple!\n",
            "epoch 45 / loss : 0.0010134164476767182 / prediction : [[4 4 3 2 0]] / true Y : tensor([[4, 4, 3, 2, 0]]) / prediction str : pple!\n",
            "epoch 50 / loss : 0.0008681646431796253 / prediction : [[4 4 3 2 0]] / true Y : tensor([[4, 4, 3, 2, 0]]) / prediction str : pple!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## II. 더 많은 데이터 문자 단위 RNN 구현\n",
        "\n",
        "### 1. 훈련 데이터 전처리"
      ],
      "metadata": {
        "id": "MXTAtFLAl8Fg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "R13qZhPbkzQd"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 긴 문장의 데이터 예제\n",
        "sentence = (\"Success is not final, failure is not fatal: It is the courage to continue that counts. In the face of adversity, the true measure of a person is not how they fall, but how they rise after falling.\")"
      ],
      "metadata": {
        "id": "B526mJk5mL89"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문자 집합 생성, 각 문자에 고유한 정수 부여\n",
        "char_set = list(set(sentence)) # 중복을 제거한 문자 집합 생성\n",
        "char_dic = {c : i for i, c in enumerate(char_set)} # 각 문자에 정수 인코딩\n",
        "print(char_dic) # 공백 또한 하나의 원소로 취급한다. (' ': 6)\n",
        "\n",
        "dic_size = len(char_dic)\n",
        "print(\"문자 집합의 크기 :\", dic_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkMtDFoVmSTT",
        "outputId": "5c4a59d1-7063-4f4f-b0b7-03ad6b840f78"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'g': 0, 'u': 1, 'n': 2, '.': 3, ',': 4, 'b': 5, ' ': 6, 'c': 7, 'S': 8, ':': 9, 'a': 10, 'v': 11, 'w': 12, 'f': 13, 'I': 14, 'p': 15, 'r': 16, 'h': 17, 'y': 18, 's': 19, 'i': 20, 'l': 21, 'e': 22, 'o': 23, 'm': 24, 't': 25, 'd': 26}\n",
            "문자 집합의 크기 : 27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터 설정\n",
        "hidden_size = dic_size # 은닉 상태의 크기를 입력의 크기와 동일하게 설정, 사용자 선택으로 다른 값을 줘도 됨.\n",
        "sequence_length = 10 # 앞서 만든 샘플을 10개 단위로 끊음.\n",
        "learning_rate = 0.1"
      ],
      "metadata": {
        "id": "f-OcZbd_sCbh"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 구성\n",
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "for i in range(0, len(sentence) - sequence_length):\n",
        "    x_str = sentence[i : i + sequence_length]\n",
        "    y_str = sentence[i + 1 : i + sequence_length + 1]\n",
        "\n",
        "    if (i >= 0 and i <= 5) or (i >= len(sentence) - sequence_length - 5 and i <= len(sentence) - sequence_length):\n",
        "        print(i, x_str, '->', y_str)\n",
        "        print()\n",
        "        if (i == 5):\n",
        "            print('. . . 중략 . . .')\n",
        "            print()\n",
        "\n",
        "    x_data.append([char_dic[c] for c in x_str]) # 각각의 정수 레이블로 저장(샘플의 각 문자들은 고유한 정수로 인코딩됨)\n",
        "    y_data.append([char_dic[c] for c in y_str]) # 마찬가지\n",
        "\n",
        "print(x_data[0]) # if you wan -> [18, 12, 6, 16, 21, 1, 6, 11, 10, 2]\n",
        "print(y_data[0]) # f you want -> [12, 6, 16, 21, 1, 6, 11, 10, 2, 23]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weistpD0s-fA",
        "outputId": "e4dd792f-7f33-44c1-a349-a9adbce5234f"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Success is -> uccess is \n",
            "\n",
            "1 uccess is  -> ccess is n\n",
            "\n",
            "2 ccess is n -> cess is no\n",
            "\n",
            "3 cess is no -> ess is not\n",
            "\n",
            "4 ess is not -> ss is not \n",
            "\n",
            "5 ss is not  -> s is not f\n",
            "\n",
            ". . . 중략 . . .\n",
            "\n",
            "181  after fal -> after fall\n",
            "\n",
            "182 after fall -> fter falli\n",
            "\n",
            "183 fter falli -> ter fallin\n",
            "\n",
            "184 ter fallin -> er falling\n",
            "\n",
            "185 er falling -> r falling.\n",
            "\n",
            "[8, 1, 7, 7, 22, 19, 19, 6, 20, 19]\n",
            "[1, 7, 7, 22, 19, 19, 6, 20, 19, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_one_hot = [np.eye(dic_size)[x] for x in x_data] # x_data를 One-Hot Encoding\n",
        "\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)\n",
        "print('훈련 데이터의 크기 : {}'.format(X.shape)) # [170 : len(sentence), 10 : sequence_length, 25 : dic_size]\n",
        "print('레이블의 크기 : {}'.format(Y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szvPjiMuv761",
        "outputId": "23f8be50-ae22-47eb-cb73-f24df6f2774b"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터의 크기 : torch.Size([186, 10, 27])\n",
            "레이블의 크기 : torch.Size([186, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 모델 구현하기\n",
        "\n",
        "- 은닉층을 두 개 이용함."
      ],
      "metadata": {
        "id": "4GKCu6DNxhfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net2(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layers): # hidden_size == dic_size\n",
        "        super(Net2, self).__init__()\n",
        "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers = layers, batch_first = True)\n",
        "        self.fc = nn.Linear(hidden_dim, hidden_dim, bias = True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, status = self.rnn(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "M34Sq_XFw9lq"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net2 = Net2(dic_size, hidden_size, 2) # num_layers = layers = 2. 은닉층 두 개 이용"
      ],
      "metadata": {
        "id": "PIfK7k5PzHSN"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net2.parameters(), learning_rate)"
      ],
      "metadata": {
        "id": "gYcuoGr9zN5D"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = net2(X)\n",
        "print(outputs.shape)\n",
        "print(outputs.view(-1, dic_size).shape) # 2D 텐서로 변환\n",
        "print(Y.shape)\n",
        "print(Y.view(-1).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKZv8OUWzY7i",
        "outputId": "b55f039e-7919-40df-b25a-f39e44c0da95"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([186, 10, 27])\n",
            "torch.Size([1860, 27])\n",
            "torch.Size([186, 10])\n",
            "torch.Size([1860])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCH = 50\n",
        "\n",
        "for epoch in range(EPOCH + 1):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net2(X) # (170, 10, 25) 크기를 가진 텐서를 매 epoch 마다 모델의 입력으로 사용\n",
        "\n",
        "    # (prediction, 정답 Y) : view를 하는 이유는 Batch 차원 제거를 위함\n",
        "    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # results의 텐서 크기는 (170, 10)\n",
        "    results = outputs.argmax(dim = 2) # 예측된 값 outputs에서 가장 높은 값을 갖는 인덱스를 찾아 저장\n",
        "    predict_str = \"\"\n",
        "\n",
        "    for j, result in enumerate(results):\n",
        "        if j == 0: # 처음에는 모든 예측 결과를 문자열로 변환, predict_str에 추가\n",
        "            predict_str += ''.join([char_set[t] for t in result])\n",
        "\n",
        "        else: # 그 다음에는 마지막 글자만 반복 추가한다.\n",
        "            predict_str += char_set[result[-1]]\n",
        "    if epoch % 5 == 0:\n",
        "        print(f\"epoch : {epoch} / model's predicted string results : {predict_str}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmuTF9O1zcG4",
        "outputId": "617e89c3-ead2-4053-8333-b7c836d49d0f"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 0 / model's predicted string results : pfnnsptptppptsp,pptp,p,pppptsptppptsp,ppppfpfpptppfpspttpttstptppntptpsptptpsppntpptps,,pptpsp,pptpt,ppptsssttp,ptpsptptpppppptftptpppptpfttpptppptsp,psptpsp,npppppfptpspsptpsp,,tptptpppfp,ppppps\n",
            "epoch : 5 / model's predicted string results :  ly   lly    ll  y y lyy l  y     y iy y y usy y  yl    y  l  y       y ly l    l  ylyIu l  y   y  iy ly l  l iyi i  yiy  y  y ly ly yI  l      yyl  o   y   y  lpsly  l  y y   yi   y t   lsly lyy\n",
            "epoch : 10 / model's predicted string results :  t t   fo t t   t te f etfot    t t t t te fe    f   eot tfe e t ee etet e t ht  oe  ef  t t e tfhtt t  t   e e t t e t      etfht  teeo   eo   fht ttf t    f tfe t   eh       fht tfet ef  teette\n",
            "epoch : 15 / model's predicted string results : e t    eo l t to slu tetslot t ue t to  le  u t  n t tot t u t t tn t uelt tfheot t ulfu t t tole tf tlue t t t t t t t  fu  uel tf tlue t t to f t tst t t tf tfu tu  tut t t tfu  lel llfeoleu  t\n",
            "epoch : 20 / model's predicted string results : ,oeo r t  not tae  n ta nuat es not ta  t, Io ns n t tou nsn t nhen  n r t ef eotoe   In tot ta n tn esun t t e n e t th ao  u n en e po n n ts not tst i e efa uu to  tot t e ef,   n,   nIaslen n\n",
            "epoch : 25 / model's predicted string results : eaao a us now tae l, batlure ts not ta  l, fn ts n t touatde totfan tnua t ad aouaa . In the ta e af adve t tyu toa t ef mo lueu af adpe aot ts not tow the tfalle tot tow thu tfusu tla  eIallen e\n",
            "epoch : 30 / model's predicted string results : urou s le not tanal, taclure ts not tac l, In ts nhe tourtge toeron tsur thef toures  In the tace tf adve aotyu the thur me sure tf aipe aoe ts not tow the  rall, but tow the  rusu tfa  eIall,ere\n",
            "epoch : 35 / model's predicted string results : urou s ns not tanal, tailure ts not tacal: It ts nhe tourtge thucont nue thet aounts. In the tace of odve orty, the thue me sure tf adperoon ts not tow the  fall, bu  how the  fuse afa r fallfnto\n",
            "epoch : 40 / model's predicted string results : uooe s in not tanal, failure ts not tacal: In ts nhe tourtge toucontinue that tounts. In the tace of adve sosy, the thue me sure of a person ts not tow the  fall, but how the  fise afi r fall,nte\n",
            "epoch : 45 / model's predicted string results : uroe s is not tanal, failure is not tatal: In ts nhe touroge toucontinue that toun s. In the tace of adve sosy, the toue me sure of a person is not iow the  rall, but how ihey rise afi r fallinge\n",
            "epoch : 50 / model's predicted string results : uroe s is not fanal, failure fs not fital: It ts the touroge toucontinue that coun s. In the tace of adversoty, the true me sure of a person ts not fow the  fall, but how they fise afi r fallin e\n"
          ]
        }
      ]
    }
  ]
}