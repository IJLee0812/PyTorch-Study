{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 머신러닝 파이토치 다루기 기초\n",
        "\n",
        "https://wikidocs.net/book/9379\n",
        "\n",
        "<br>\n",
        "\n",
        "파이토치는 다른 머신러닝 프레임워크들과 비교해서 상대적으로 간단한 인터페이스를 제공하며, 빠르고 효율적인 계산 그래프를 통해 머신러닝 모델을 구축할 수 있습니다.\n",
        "\n",
        "파이토치는 파이썬 언어를 기반으로 하며, NumPy와 유사한 구문을 사용하여 텐서 연산을 수행합니다. 이를 통해 파이썬을 사용하여 머신러닝 모델을 쉽게 구현할 수 있습니다. 또한 파이토치는 자동 미분(automatic differentiation) 기능을 지원하여, 모델을 훈련할 때 역전파 알고리즘을 간단하게 구현할 수 있습니다."
      ],
      "metadata": {
        "id": "MiIxj6jXUUSL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqO6mm5mR-rh",
        "outputId": "cb655643-7dd2-4fc0-cd91-a13642de07d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]])\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]], dtype=torch.int32)\n",
            "tensor([1, 2, 3])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# 파이토치 tensor : 다차원 배열 = Numpy ndarray와 유사한 API 제공, GPU 이용한 연산도 지원.\n",
        "\n",
        "# tensor() 메소드 : 파이썬 list나 Numpy 배열을 PyTorch 텐서로 변환해주는 역할\n",
        "\n",
        "# torch.tensor(data, dtype = None, device = None, requires_grad = False)\n",
        "\n",
        "data = [[1, 2, 3], [4, 5, 6]]\n",
        "x = torch.tensor(data, dtype = torch.float32)\n",
        "print(x)\n",
        "x = torch.tensor(data, dtype = torch.int32)\n",
        "print(x)\n",
        "\n",
        "x = torch.tensor([1, 2, 3])\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = torch.zeros((2, 3, 4)) # 모든 원소가 0인 3차원 텐서 (2x3x4)\n",
        "print(z)\n",
        "\n",
        "z = torch.ones((3, 4, 5)) # 모든 원소가 1인 4차원 텐서 (3x4x5)\n",
        "print(z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIqq0EljSU-X",
        "outputId": "db698761-e997-4de1-dc72-3d76dbcbafba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0.]]])\n",
            "tensor([[[1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1.]],\n",
            "\n",
            "        [[1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1.]],\n",
            "\n",
            "        [[1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1.]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## .rand() 함수 : Uniform distribution에서 난수 생성, 따라서 최솟값과 최댓값 사이의 값들이 동일한 확률로 선택되는 분포. 0과 1 사이의 값이 동일한 확률로 선택되는 무작위 값 생성\n",
        "\n",
        "## .rand'n'() 함수 : Gaussian distribution에서 난수 생성, 평균이 0이고 분산이 1인 정규분포에서 무작위 값 생성\n",
        "\n",
        "### 정규분포는 자연계에서 많이 발견되며, 중심극한정리(CLT)에 따라 다른 분포로 근사할 수 있다."
      ],
      "metadata": {
        "id": "uN33spbhTg7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand((2, 3))\n",
        "print(x)\n",
        "nx = torch.rand((2, 3))\n",
        "print(nx)\n",
        "\n",
        "print()\n",
        "mean = 2.0\n",
        "sd = 1.5 # 표준편차\n",
        "size = (3, 4)\n",
        "samples = torch.randn(size) * sd + mean # 평균이 2이고 표준편차가 1.5인 정규분포에서 무작위 샘플 생성\n",
        "print(samples)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tk8hpPy2VKC1",
        "outputId": "7cefaf41-8c5c-4ee8-e864-da679d59d64e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4507, 0.5448, 0.7091],\n",
            "        [0.7965, 0.8401, 0.6817]])\n",
            "tensor([[0.0026, 0.4583, 0.0590],\n",
            "        [0.1031, 0.2564, 0.3196]])\n",
            "\n",
            "tensor([[4.9636, 2.5834, 1.8368, 3.7382],\n",
            "        [2.0421, 0.6199, 2.6666, 1.9489],\n",
            "        [3.1620, 0.9542, 3.6176, 2.4695]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## .tensor와는 다르게 대문자로 시작하는 .Tensor는 기본적으로 FloatTensor와 동일(IntTensor, BoolTensor)\n",
        "\n"
      ],
      "metadata": {
        "id": "fzHvOr8BVYG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.Tensor([1, 2, 3])\n",
        "print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LcQVaEeVTHX",
        "outputId": "55e2b8d2-0ad9-46b3-f5f6-7a2ee85713cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 2., 3.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## torch.from_numpy()는 NumPy 배열을 PyTorch Tensor로 변환한다. 메모리 상 동일한 위치를 참조한다.(torch.tensor()와 다르다)\n",
        "\n",
        "* 데이터 크기가 크고 계산비용이 많이 드는 경우에는 NumPy 배열과 Tensor를 공유하여 메모리를 절약할 수 있다.\n",
        "\n"
      ],
      "metadata": {
        "id": "3RMVJ-FSWRGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "a_np = np.array([1, 2, 3])\n",
        "a_tensor = torch.from_numpy(a_np)\n",
        "\n",
        "a_tensor[0] = -1 ; print(a_np) # NumPy 배열 출력결과 : [-1, 2, 3]으로 index 0 원소가 같이 변경됨"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BI1Z3gR0V42l",
        "outputId": "628ed3bb-02b7-4a5b-dc9c-792a58defc56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1  2  3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- numel() 메소드를 사용하여 크기를 출력할 수 있다.\n",
        "\n",
        "- device 속성은 CPU를 기본값으로 가지며, torch.device를 사용하여 GPU를 지정할 수 있다.\n",
        "\n",
        "- size() 속성을 사용하여 모양을 출력하고, view()를 사용하여 모양을 변경한다.\n",
        "\n",
        "- PyTorch의 size() 메소드는 텐서의 크기를 반환하는 함수이다. 반환되는 크기 정보를 튜플 형태로 제공한다. <-> NumPy 배열의 경우 size() 메소드는 총 원소 개수를 반환한다."
      ],
      "metadata": {
        "id": "ogR5LCHMHLWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cpu')\n",
        "x = torch.tensor([1, 2, 3], device = device)\n",
        "print(x.device) # cpu 출력됨\n",
        "\n",
        "x = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
        "print(x.shape, x.size())\n",
        "\n",
        "y = x.reshape(2, 3) # 모양 변경하는 함수\n",
        "z = y.view(1, 6)\n",
        "print(y.size())\n",
        "\n",
        "print(y)\n",
        "print(z)"
      ],
      "metadata": {
        "id": "E5Os-1XlYeL2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "636665fb-c89a-49df-97e9-5f0d94ff447f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "torch.Size([3, 2]) torch.Size([3, 2])\n",
            "torch.Size([2, 3])\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "tensor([[1, 2, 3, 4, 5, 6]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 텐서 생성\n",
        "a = torch.tensor([[1, 2], [3, 4]])\n",
        "b = torch.tensor([[5, 6], [7, 8]])\n",
        "\n",
        "# 덧셈\n",
        "c = a + b\n",
        "print(c)\n",
        "\n",
        "# 뺄셈\n",
        "d = a - b\n",
        "print(d)\n",
        "\n",
        "# 곱셈\n",
        "e = a * b\n",
        "print(e)\n",
        "\n",
        "# 나눗셈\n",
        "f = a / b\n",
        "print(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpd8elxPIEXl",
        "outputId": "e6e935dd-38ce-41ff-c407-5984427a0979"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 6,  8],\n",
            "        [10, 12]])\n",
            "tensor([[-4, -4],\n",
            "        [-4, -4]])\n",
            "tensor([[ 5, 12],\n",
            "        [21, 32]])\n",
            "tensor([[0.2000, 0.3333],\n",
            "        [0.4286, 0.5000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 행렬곱 : torch.mm() 메소드(matrix multiplication)\n",
        "\n",
        "### 내적(Dot Product) : torch.dot() 메소드\n",
        "- 일차원 벡터(1 x n or n x 1)만 내적 연산 계산가능"
      ],
      "metadata": {
        "id": "KX8ctHwyI5tw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 행렬곱\n",
        "g = torch.mm(a, b)\n",
        "print(g)\n",
        "\n",
        "# 내적\n",
        "# h = torch.dot(a, b) # dot product -> 불가능\n",
        "h = torch.dot(torch.tensor([1, 2]), torch.tensor([3, 4])) # dot product\n",
        "print(h)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d2zRIYzI4kh",
        "outputId": "f02f4f85-0a73-4ead-d216-0c03a81ab4b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[19, 22],\n",
            "        [43, 50]])\n",
            "tensor(11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 요소별 최대값 계산\n",
        "i = torch.tensor([[1, 2], [3, 4]])\n",
        "j = torch.tensor([[4, 3], [2, 1]])\n",
        "k = torch.max(i, j) # 텐서 크기가 동일해야 함에 유의한다.\n",
        "print(k)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "scvTHIpIJVgb",
        "outputId": "e9852001-9b4a-418f-d002-b754dd1f0845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[4, 3],\n",
            "        [3, 4]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 차원 축소\n",
        "l = torch.tensor([[[1, 2], [3, 4], [5, 6]], [[1, 2], [3, 4], [5, 6]]])\n",
        "m = torch.sum(l, dim=0) # 차원 하나를 각각의 원소들을 summation 시켜서 축소시킴.\n",
        "print(m)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpcAtACNKY3K",
        "outputId": "9a6ae741-8029-4f92-f206-c1bf3ff09829"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 2,  4],\n",
            "        [ 6,  8],\n",
            "        [10, 12]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 전치\n",
        "n = torch.tensor([[1, 2], [3, 4]])\n",
        "o = torch.transpose(n, 0, 1)\n",
        "print(o)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9QlpoNLKsB7",
        "outputId": "22b689db-9d86-4b3b-e885-8367612efb55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 3],\n",
            "        [2, 4]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 인덱싱\n",
        "p = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
        "q = p[1, 0]\n",
        "print(q) # tensor(3)\n",
        "r = p[2, 1]\n",
        "print(r)\n",
        "\n",
        "# 슬라이싱\n",
        "r = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "s = r[:, :2]\n",
        "print(s)\n",
        "\n",
        "# 조건부 인덱싱\n",
        "t = torch.tensor([1, 2, 3, 4, 5])\n",
        "u = t[t > 3]\n",
        "print(u)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIRbOmNpLMev",
        "outputId": "29645349-a900-4707-ed1b-9d62c50fd1e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3)\n",
            "tensor(6)\n",
            "tensor([[1, 2],\n",
            "        [4, 5],\n",
            "        [7, 8]])\n",
            "tensor([4, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 텐서 차원 변경\n",
        "- unsqueeze() : 지정한 dimension 자리에 size가 1인 빈 공간을 채워주면서 차원을 확장함.\n",
        "- squeeze() : 차원이 '1'인 차원을 제거해준다. 따로 차원을 설정하지 않으면 '1'인 차원을 모두 제거하며 차원을 설정해주면 그 차원만 제거한다.\n",
        "- reshape()"
      ],
      "metadata": {
        "id": "KEQa4KKniRfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# unsqueeze()\n",
        "x = torch.ones(3, 5, 7)\n",
        "\n",
        "# 1번과 2번 사이에 dimension 추가\n",
        "x1 = x.unsqueeze(dim = 0)\n",
        "print(x1.shape)\n",
        "# 마지막 자리에 dimension 추가\n",
        "x2 = x.unsqueeze(dim = -1)\n",
        "print(x2.shape)\n",
        "# 인덱스 1에 dimension 추가\n",
        "x3 = x.unsqueeze(dim = 1)\n",
        "print(x3.shape)\n",
        "# 오류가 발생하는 경우 : 원래 텐서의 차원보다 큰 숫자의 차원을 삽입하는 경우, 해당 위치에 dimension 추가 불가능하므로 IndexError 발생\n",
        "# x4 = x.unsqueeze(dim = 4) -> IndexError : Dimension out of range.\n",
        "\n",
        "# squeeze()\n",
        "print()\n",
        "x = torch.randn(1, 1, 20, 128)\n",
        "print(x)\n",
        "x1 = x.squeeze()\n",
        "print(x1.shape)\n",
        "x2 = x.squeeze(dim = 1) # 차원 설정\n",
        "print(x2.shape)\n",
        "\n",
        "# reshape()\n",
        "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "x_re = x.reshape(3, 2)\n",
        "print(x_re)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLUmZfytiV8f",
        "outputId": "de9aeb81-2ea0-46bc-ca04-0056d3fd28d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 5, 7])\n",
            "torch.Size([3, 5, 7, 1])\n",
            "torch.Size([3, 1, 5, 7])\n",
            "\n",
            "tensor([[[[ 1.2060, -0.2139,  0.7841,  ...,  1.5184, -0.7158,  1.1409],\n",
            "          [-0.0733,  0.4614,  0.7135,  ..., -0.1593,  0.7741, -1.3919],\n",
            "          [-1.2500,  0.6603, -0.0510,  ..., -1.0209, -1.0377,  0.5654],\n",
            "          ...,\n",
            "          [ 0.3363, -0.6450, -0.9256,  ..., -0.0890, -0.0278,  0.7407],\n",
            "          [ 0.9264, -1.0432,  1.4146,  ...,  1.0120,  0.1333, -0.4418],\n",
            "          [ 1.0743, -1.0548, -0.2440,  ...,  0.8768,  0.7371,  0.4665]]]])\n",
            "torch.Size([20, 128])\n",
            "torch.Size([1, 20, 128])\n",
            "tensor([[1, 2],\n",
            "        [3, 4],\n",
            "        [5, 6]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 텐서 요소 정렬\n",
        " - sort() 함수 : 주어진 텐서를 정렬하고, 두 개의 텐서를 리턴한다. 첫 번째 텐서는 정렬된 결과를 나타내고, 두 번째 텐서는 원래 텐서의 각 요소가 정렬된 결과에서 차지하는 위치를 나타낸다."
      ],
      "metadata": {
        "id": "ufHWAhlymiIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1, 2, 4, 3])\n",
        "\n",
        "a, b = torch.sort(x)\n",
        "print(a, b)\n",
        "\n",
        "a, b = torch.sort(x, descending = True)\n",
        "print(a, b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZUsk7kChs1J",
        "outputId": "30d5a447-cf5c-483a-b63f-1682cd4e852e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 3, 4]) tensor([0, 1, 3, 2])\n",
            "tensor([4, 3, 2, 1]) tensor([2, 3, 1, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 텐서 병합"
      ],
      "metadata": {
        "id": "-UfTn2qzn6kc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "arr1 = np.array([[1, 2], [3, 4]])\n",
        "arr2 = np.array([[5, 6], [7, 8]])\n",
        "\n",
        "a1 = torch.tensor(arr1)\n",
        "a2 = torch.tensor(arr2)\n",
        "\n",
        "print(torch.cat((a1, a2), axis = 1))\n",
        "print(torch.vstack((a1, a2))) # v : vertical. 수직\n",
        "print(torch.hstack((a1, a2))) # h : horizontal. 수평"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxoWsd2im-fc",
        "outputId": "f0d1af71-6d0c-46cd-e020-9e318964d6a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2, 5, 6],\n",
            "        [3, 4, 7, 8]])\n",
            "tensor([[1, 2],\n",
            "        [3, 4],\n",
            "        [5, 6],\n",
            "        [7, 8]])\n",
            "tensor([[1, 2, 5, 6],\n",
            "        [3, 4, 7, 8]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 연산_유용한 함수들\n",
        "\n",
        "- torch.eq() : 두 개의 입력 텐서를 받아, 두 텐서의 \"원소 간 동등성\"을 비교하여 bool 값으로 채워진 같은 모양(shape)의 텐서를 리턴함.\n",
        "\n",
        "- torch.softmax(input, dim = None, dtype = None) : 소프트맥스 함수, 입력값을 확률(0~1) 형태로 변환해주는 함수로, 다중(클래스) 분류(Multiclass Classification) 문제에서 예측된 점수를 확률로 변환하는 데 사용된다. dim = None인 경우 입력 텐서의 마지막 차원이 사용된다."
      ],
      "metadata": {
        "id": "JJA09E8OpXsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.eq()\n",
        "a = torch.tensor([1, 2, 3])\n",
        "b = torch.tensor([1, 2, 3])\n",
        "c = torch.tensor([1, 2, 4])\n",
        "res1 = torch.eq(a, b)\n",
        "res2 = torch.eq(a, c)\n",
        "print(res1)\n",
        "print(res2)\n",
        "\n",
        "# torch.softmax()\n",
        "a = torch.tensor([1.0, 2.0, 3.0])\n",
        "softmax_output = torch.softmax(a, dim = 0)\n",
        "print(softmax_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDu1KKfFoKms",
        "outputId": "66361707-f22e-45d0-fcef-55d6d81fb4da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([True, True, True])\n",
            "tensor([ True,  True, False])\n",
            "tensor([0.0900, 0.2447, 0.6652])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 인플레이스 연산\n",
        "-> \"원본 텐서의 값을 변경\"하며, 메모리를 절약할 수 있는 장점이 있다. PyTorch에서 많이 사용되는 연산 중 하나이다.\n",
        "\n",
        "- add_\n",
        "sub_\n",
        "mul_\n",
        "div_\n",
        "pow_\n",
        "sqrt_\n",
        "round_\n",
        "floor_\n",
        "ceil_\n",
        "clamp_\n",
        "fill_\n",
        "\n",
        "- '_'가 연산자 뒤에 붙는 메소드를 사용하여 수행할 수 있다.\n",
        "\n",
        "- 인플레이스 연산을 사용하기 전 반드시 복사본을 생성하여 원본 데이터를 보호해야 한다."
      ],
      "metadata": {
        "id": "NrrpKwocLBUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([1, 2, 3])\n",
        "b = a\n",
        "a.add_(torch.tensor([1, 1, 1]))\n",
        "print(a) ; print(b)\n",
        "\n",
        "# fill() 함수 : 텐서의 모든 요소를 지정된 값으로 채운다. \"인플레이스 연산임\"\n",
        "x = torch.zeros(3, 3)\n",
        "print(x)\n",
        "x.fill_(1)\n",
        "print(x)\n",
        "\n",
        "# <-> full() 함수 : 주어진 크기(행렬 사이즈)와 지정된 값으로 새로운 텐서를 생성한다. \"인플레이스 연산이 아님\" = 새로운 텐서를 생성함.\n",
        "x = torch.full((3, 3), 5)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICektPdyJ7bJ",
        "outputId": "cf88e14b-f7b4-4158-c0bf-237a35f5d7d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 3, 4])\n",
            "tensor([2, 3, 4])\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "tensor([[5, 5, 5],\n",
            "        [5, 5, 5],\n",
            "        [5, 5, 5]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## - - - Dataset과 DataLoader - - -\n",
        "\n",
        "-> 데이터를 효율적으로 처리하고 모델 학습에 사용하기 위한 데이터 파이프라인을 구축하기 위해 사용됨.\n",
        "\n",
        "- PyTorch에서 제공하는 데이터 로딩 및 전처리 기능을 제공하며, 모델 학습에 필요한 데이터를 'batch' 단위로 처리할 수 있도록 도와준다."
      ],
      "metadata": {
        "id": "vKqweObYMcaI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "-> Dataset 클래스는 사용자가 정의하는 데이터셋에 대한 인터페이스를 제공함. 사용자는 Dataset 클래스를 상속받아 자신이 사용하고자 하는 데이터셋에 맞게 커스텀 데이터셋 클래스를 구현해야 함. 데이터셋의 샘플을 가져오고, 전처리를 수행하며, 샘플의 개수를 반환하는 등의 기능을 제공. 아래 메소드들의 구현 필수.\n",
        "- len() : 데이터셋의 전체 샘플 개수를 반환하는 메소드로, 정수 값을 리턴한다.\n",
        "- getitem(idx) : 인덱스 idx에 해당하는 샘플을 가져오는 메소드로, 인덱스에 따라 샘플 데이터와 레이블(label) 등을 리턴한다.\n",
        "<br><br>\n",
        "- 이미지, 텍스트, 오디오, 비디오 등 다양한 유형의 데이터에 대하여 Dataset 클래스를 구현하여 사용할 수 있다. 직접 커스텀 데이터셋 클래스 구현 시, 유연한 처리가 가능해지고 모델 학습에 필요한 데이터를 원하는 형태로 로딩하고 전처리할 수 있다."
      ],
      "metadata": {
        "id": "gU4QHgiRNRQh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### torchvision.transforms : PyTorch에서 이미지 데이터의 전처리 및 데이터 증강(augmentation)을 위해 제공하는 모듈.\n",
        "\n",
        "- Resize: 이미지의 크기를 조절합니다.\n",
        "- RandomResizedCrop: 이미지를 무작위로 자르고 크기를 조절합니다.\n",
        "- RandomHorizontalFlip: 이미지를 무작위로 수평으로 뒤집습니다.\n",
        "- RandomVerticalFlip: 이미지를 무작위로 수직으로 뒤집습니다.\n",
        "- ToTensor: 이미지를 텐서로 변환합니다.\n",
        "- Normalize: 이미지를 정규화합니다.\n",
        "- ColorJitter: 이미지의 색상을 무작위로 조정합니다.\n",
        "- RandomRotation: 이미지를 무작위로 회전합니다.\n",
        "- RandomCrop: 이미지를 무작위로 자릅니다.\n",
        "- Grayscale: 이미지를 흑백으로 변환합니다.\n",
        "- RandomSizedCrop: 이미지를 무작위로 자르고 크기를 조절합니다."
      ],
      "metadata": {
        "id": "wcF62NRrIqLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as T # 이미지 전처리 작업 정의\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image # PIL 라이브러리 : https://wikidocs.net/213022\n",
        "\n",
        "img_T = T.Compose([\n",
        "    T.Resize((256, 256)),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.5, ), (0.5, ))\n",
        "])\n",
        "\n",
        "# image = Image.open('image.jpg') ; imge = img_T(image)\n",
        "\n",
        "class CustomImageDataset(Dataset): # Dataset 클래스를 상속받는다.\n",
        "\n",
        "    \"\"\"\n",
        "    커스텀 이미지 데이터셋 클래스의 생성자, 이미지 파일의 경로와 레이블을 입력받아 이미지 데이터와 레이블을 텐서로 변환하여 리턴함.\n",
        "    Args:\n",
        "        file_paths (list): 이미지 파일 경로의 리스트\n",
        "        labels (list): 이미지 레이블의 리스트\n",
        "        transform (callable, optional): 이미지에 적용할 전처리 함수\n",
        "    \"\"\"\n",
        "    # def __init__(self, file_paths, labels, transform = None, img_T = None):\n",
        "    #     self.file_paths = file_paths\n",
        "    #     self.labels = labels\n",
        "    #     self.transform = transform\n",
        "\n",
        "\n",
        "    # def __len__(self):\n",
        "    #     return len(self.file_paths) # 데이터셋의 전체 샘플 개수를 리턴\n",
        "\n",
        "    # def __getitem__(self, idx):\n",
        "    #     # 이미지 파일 로딩\n",
        "    #     image = Image.open(self.file_paths[idx])\n",
        "\n",
        "    #     # 이미지에 전처리 함수 적용(Resize, RandomCrop, ToTensor 등)\n",
        "    #     if self.transform is not None:\n",
        "    #         image = self.transform(image)\n",
        "\n",
        "    #     # 이미지 레이블을 텐서로 변환\n",
        "    #     label = torch.tensor(self.labels[idx])\n",
        "\n",
        "    #     return image, label\n",
        "    def __init__(self, file_list, label_list, img_T=None):\n",
        "        self.file_list = file_list\n",
        "        self.label_list = label_list\n",
        "        self.img_T = img_T\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.file_list[idx])\n",
        "        label = self.label_list[idx]\n",
        "\n",
        "        if self.img_T is not None:\n",
        "            image = self.img_T(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "# 상기 클래스 인스턴스화 시, img_T 인자에 전처리 작업(Composing)을 적용하여 이미지 데이터를 전처리할 수 있다.\n",
        "# dataset = CustomImageDataset(file_paths, labels, img_T = img_T)"
      ],
      "metadata": {
        "id": "j4wWupHqLQiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torchvision.utils.save_image() : 이미지 텐서를 파일로 저장하는 함수. 주로 딥러닝 모델이 생성한 이미지 저장 시 사용됨.\n",
        "# torchvision.utils.save_image(tensor, filename, nrow=8, padding=2, normalize=False, range=None, scale_each=False, pad_value=0)\n",
        "\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "# Create a tensor of shape (3, 64, 64) representing a single RGB image\n",
        "img = torch.randn(3, 64, 64)\n",
        "\n",
        "# Save the image to a file\n",
        "vutils.save_image(img, 'my_image.png')"
      ],
      "metadata": {
        "id": "1DcjOZrzGSsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataLoader\n",
        "-> PyTorch에서 제공하는 데이터 로딩 유틸리티로, 모델 학습 시에 데이터를 배치(batch) 단위로 로딩해와 효율적인 학습을 가능하게 해주는 클래스이다.\n",
        "<br>\n",
        "<br>\n",
        "DataLoader의 중요한 인자들과 기능\n",
        "- dataset : 데이터를 로드할 데이터셋 객체를 지정합니다. 이는 torch.utils.data.Dataset 클래스를 상속받은 사용자 정의 데이터셋 클래스를 사용하거나, torchvision의 내장 데이터셋 클래스를 사용할 수 있습니다. (기본값: None)\n",
        "- batch_size: 한 번에 로드할 배치(batch)의 크기를 지정합니다. 작은 배치 크기는 더 많은 메모리를 사용하지만 더 자주 모델이 업데이트되고 더 높은 학습 속도를 제공합니다. (기본값: 1)\n",
        "- shuffle: 데이터를 섞을지 여부를 지정합니다. True로 설정할 경우, 데이터가 매 에폭(epoch)마다 섞여서 모델이 각 배치에서 다양한 데이터를 학습하도록 도와줍니다.(기본값: False)\n",
        "- num_workers: 데이터 로딩에 사용할 워커(worker)의 수를 지정합니다. '병렬 처리'를 통해 데이터 로딩 속도를 향상시키는 데 사용됩니다.(기본값: 0)\n",
        "- pin_memory: GPU 메모리에 데이터를 고정할지 여부를 지정합니다. GPU를 사용하는 경우 True로 설정하면 데이터가 CPU와 GPU 간에 더 빠르게 복사되어 학습 속도를 향상시킬 수 있습니다. (기본값: False)\n",
        "- collate_fn: 배치를 생성하기 전에 데이터를 결합하는 함수를 지정합니다. 기본값은 None이며, 데이터셋이 출력하는 원시 데이터의 리스트를 배치로 결합합니다. 필요에 따라 사용자 정의 결합 함수를 지정하여 배치를 구성할 수 있습니다.(기본값: None)\n",
        "- drop_last: 마지막 배치의 크기가 batch_size보다 작을 경우, 해당 배치를 무시할지 여부를 지정합니다. True로 설정할 경우 마지막 배치를 무시합니다.(기본값: False)\n",
        "\n",
        "-> 위 인자들을 조정하여 데이터 로딩의 성능을 최적화할 수 있다. 또한 DataLoader는 Iterable한 객체로, for 루프를 사용하여 데이터를 배치 단위로 간단하게 로드할 수 있다. 배치 단위로 로드된 데이터는 모델에 전달되어 학습이 이루어진다.\n",
        "\n",
        "---\n",
        "<br>\n",
        "\n",
        "### pin_memory\n",
        "\n",
        "-> GPU를 사용하는 경우 데이터를 GPU 메모리에 고정(pinned)시키는 옵션이다. 이를 통해 CPU와 GPU 간 데이터 복사가 최소화되어 데이터 로딩속도를 향상시킬 수 있게 된다.\n",
        "\n",
        "- 작동방식\n",
        "    \n",
        "    1. DataLoader가 데이터를 로딩하여 CPU 메모리에 올린다.\n",
        "    2. pin_memory = True인 경우, CPU 메모리에 있는 데이터를 GPU 메모리에 고정시킨다.\n",
        "    3. 모델이 데이터를 GPU에서 사용 시, 데이터는 GPU 메모리에서 직접 읽히므로 차후 CPU와 GPU 간 데이터 복사가 생략되게 된다.\n",
        "\n",
        "이러한 과정을 통해 데이터 로딩 속도가 향상되어, 학습 속도를 탁월하게 개선시킬 수 있다.\n",
        "\n",
        "- 주의사항 : GPU Memory를 고려하여 메모리 용량을 조절하는 것이 중요하다. 큰 용량의 데이터를 로딩할 경우 pinned 시 GPU 메모리 부족으로 인한 Out Of Memory(OOM) 에러가 발생할 수 있다. 따라서 \"GPU의 메모리 용량과 데이터의 크기를 고려\"해야 한다."
      ],
      "metadata": {
        "id": "g762NcUZKJxn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 만들기\n",
        "-> PyTorch에서 모델을 만드는 방법은 크게 두 가지가 있다.\n",
        "\n",
        "- nn.Module 클래스를 '상속'하여 모델 클래스를 정의하는 방법\n",
        "\n",
        "- nn.Sequential 클래스를 사용하여 모델을 정의하는 방법"
      ],
      "metadata": {
        "id": "WA_QsbZiFToZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nn.Module 클래스를 상속하여 모델 클래스를 정의하는 예시\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        # 모델의 레이어들을 정의하고 초기화\n",
        "        self.layer1 = nn.Linear(784, 256)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.layer2 = nn.Linear(256, 64)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.layer3 = nn.Linear(64, 10)\n",
        "\n",
        "    # 모델의 순전파(forward propagation) 동작 구현. nn.Module 클래스 상속 시 명시적으로 forward() 메소드를 구현해야 한다.\n",
        "    def forward(self, x): # 초기 파라미터 x는 입력으로 사용됨\n",
        "        x = self.layer1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.layer3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "pJIPuFfpBJL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nn.Sequential 클래스를 사용하여 모델을 정의하는 방법\n",
        "# nn.\"Sequential\" 사용하는 경우에는 자동적으로 순차적으로(forward) 레이어들이 연결되어, 순전파 동작이 정의된다.\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(784, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, 10)\n",
        ")\n",
        "\n",
        "# nn.Module 상속한 클래스 내부에서 nn.Sequentual 사용하여 레이어 조합하는 예제\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "\n",
        "        # nn.Sequential을 사용하여 레이어들을 조합\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(784, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # nn.Sequential로 정의한 레이어들의 순전파 동작이 자동으로 수행됨\n",
        "        x = self.layers(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "_YVyKe0yGm8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## nn.BatchNorm1d / 2d\n",
        "-> Batch Normalization을 수행하는 데 사용되는 모듈이다. 입력 데이터를 평균과 표준편차로 정규화하여, 모델이 더 잘 수렴하도록(Accuracy 향상) 돕는 방법 중 하나이다.\n",
        "\n",
        "- nn.BatchNorm1d는 \"1차원 입력\"에 대해서만 정규화를 수행하며, 생성자에서 num_features 인자를 받는다. 이 인자는 입력 데이터의 채널 수를 나타낸다.\n",
        "\n",
        "nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "\n",
        "<br>\n",
        "\n",
        "- nn.BatchNorm2d는 \"2차원 이미지 데이터\"에 대한 배치 정규화를 적용할 수 있다.\n",
        "nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"
      ],
      "metadata": {
        "id": "5MMq5_zzHpRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nn.Linear 모듈을 이용하여 입력/출력층을 구성하고, nn.BatchNorm1d 모듈을 이용하여 은닉층 출력에 배치 정규화 적용.\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(100, 50),\n",
        "\n",
        "            nn.BatchNorm1d(num_features = 50), # 입력 데이터의 채널 수를 지정하는 인자로, 반드시 지정해야 함에 유의한다. activation function 통과 전 적용한다.\n",
        "\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(50, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layers(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Q2VaXfAMG9ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## nn.Conv1d / 2d\n",
        "-> PyTorch에서 Convolution Layer를 정의하는 모듈이다.\n",
        "\n",
        "- nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
        "<br>\n",
        "> dilation(int 또는 tuple, optional) : 딜레이션 비율이다. 필터의 간격을 더 크게 두어 더 넓은 영역의 정보를 가져오는 데 사용된다. 기본값은 1이며, 값이 터질수록 필터의 영역이 더 넓어지게 된다.\n",
        "> groups(int, optional) : 입력 및 출력 채널을 묶는(그루핑) 개수이다. 기본값은 1이며 값이 크면 채널 간 관련성을 줄이는 효과가 있다.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "- 1차원 컨볼루션은 입력 데이터의 한 방향(시계열 데이터에서 시간 축)으로 컨볼루션 연산을 수행한다."
      ],
      "metadata": {
        "id": "GALdClH0Lyak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력 데이터의 크기 : (배치 크기, 채널, 시퀀스 길이)\n",
        "input_size = (16, 3, 100)\n",
        "\n",
        "# 1차원 컨볼루션 레이어 정의\n",
        "conv1d = nn.Conv1d(in_channels = 3, out_channels = 16, kernel_size = 3, stride = 1, padding = 1) # in/out_channels, kernel_size는 필수 argument\n",
        "\n",
        "# 입력 데이터 생성\n",
        "input_data = torch.randn(input_size)\n",
        "\n",
        "# 컨볼루션 연산 수행\n",
        "output = conv1d(input_data)\n",
        "\n",
        "# 출력 데이터의 크기 : (배치 크기, 출력 채널, 출력 시퀀스 길이)\n",
        "print(\"output size : \", output.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzOrfwuSJIUX",
        "outputId": "199f0d5c-5927-413d-b47d-c71f45e49069"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output size :  torch.Size([16, 16, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- nn.Conv2d : 이미지나 2D 데이터의 \"특징 추출 = feature map\"에 주로 사용되며, CNN에서 핵심적인 레이어이다.\n",
        "<br>\n",
        "nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)"
      ],
      "metadata": {
        "id": "GXvj4sW0PzjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력 데이터의 크기가 (64, 3, 32, 32)인 4D 텐서를 입력으로 받아 3개의 입력 채널을 64개의 출력 채널로 변환하는 Convolution 연산을 수행하고 출력 데이터의 크기를 출력한다.\n",
        "\n",
        "# 입력 데이터의 크기 : (배치 크기, 채널, 높이, 너비)\n",
        "input_size = (64, 3, 32, 32)\n",
        "\n",
        "# 입력 데이터 생성\n",
        "input_data = torch.randn(input_size)\n",
        "\n",
        "# Conv2d 레이어 정의 (in_channels 사이즈 맞춰줘야 한다.)\n",
        "conv = nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = 3, stride = 1, padding = 1) # out_channels == feature map의 구성요소 개수\n",
        "\n",
        "# 컨볼루션 연산 수행\n",
        "output = conv(input_data)\n",
        "\n",
        "# 출력 데이터의 크기 : (배치 크기, 출력 채널, 출력 높이, 출력 너비)\n",
        "print(\"output size : \", output.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcBq6CveOuW8",
        "outputId": "b74a03ca-542e-472e-bcd4-eaf978e1511a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output size :  torch.Size([64, 64, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## nn.Flatten()\n",
        "-> PyTorch의 텐서를 1차원으로 평탄화(flatten)하는 클래스. 다차원 텐서를 1차원으로 변환하여 MLP 등의 신경망 레이어에 입력으로 제공할 수 있게 해준다.\n",
        "- 입력 텐서를 평탄화하는 작업을 수행하므로, 별도으 ㅣ인수나 기본값이 없다."
      ],
      "metadata": {
        "id": "D_Ht1wUNBa7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = (64, 3, 32, 32)\n",
        "input_data = torch.randn(input_size)\n",
        "flatten = nn.Flatten\n",
        "input_flatten = flatten(input_data)"
      ],
      "metadata": {
        "id": "eeFz1WAlBVS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## nn.Linear\n",
        "-> 선형 변환(linear transformation)을 수행하는 클래스로, FC Layer이다. Weight와 Bias를 학습하며, 입력 텐서를 선형 변환하여 출력 텐서를 생성한다. 입력 텐서와 가중치 행렬의 행렬 곱을 계산하고 편향을 더하는 연산으로 이뤄진다.\n",
        "\n",
        "nn.Linear 클래스의 생성자 __init__에는 다음과 같은 인수들이 있다.\n",
        "- in_features(int) : 입력 텐서의 크기(차원 또는 특성의 개수)\n",
        "- out_features(int) : 출력 텐서의 크기\n",
        "- bias(bool, optional) : 편향(bias)을 사용할지의 여부를 지정함. 기본값은 True"
      ],
      "metadata": {
        "id": "WqH-cl4bDUd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력 텐서의 크기가 10이고 출력 텐서의 크기가 20인 선형 변환을 수행하는 nn.Linear 모듈 생성\n",
        "linear = nn.Linear(10, 20)\n",
        "\n",
        "# 입력 텐서 생성 (크기가 10인 벡터)\n",
        "input_tensor = torch.randn(1, 10)\n",
        "print(input_tensor)\n",
        "\n",
        "# 선형 변환 수행(입력 텐서를 출력 텐서로 선형 변환)\n",
        "output_tensor = linear(input_tensor)\n",
        "\n",
        "print(\"Input Tensor Size: \", input_tensor.size())\n",
        "print(\"Output Tensor Size: \", output_tensor.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luoyOQueB3QT",
        "outputId": "3ccfd821-5b15-4be1-cbd0-2584ad5e9a9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.2241,  0.5693, -0.2266,  1.4295,  0.1522,  0.2327,  0.3796, -0.4424,\n",
            "          1.3747,  0.7451]])\n",
            "Input Tensor Size:  torch.Size([1, 10])\n",
            "Output Tensor Size:  torch.Size([1, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## nn.MaxPool1d / 2d\n",
        "-> PyTorch 라이브러리에서 제공하는 MaxPooling 연산을 수행하는 클래스. Feature Map의 공간 차원을 줄이는 역할을 수행한다. Convolution 연산을 통해 추출된 특징들을 압축하고, 불필요한 저옵를 줄이는 효과를 얻는다.\n",
        "\n",
        "nn.MaxPool1d/2d(kernel_size, stride, padding)\n",
        "\n",
        "- kernel_size : Pooling Window의 크기를 나타내는 정수 값 또는 튜플. 일반적으로 '2' 또는 3과 같은 작은 정수 값이 사용됨.(Conv1d / 2d와 혼동 주의.)\n",
        "- stride : 풀링 윈도우의 이동 간격을 나타내는 정수 값 또는 튜플. 일반적으로 \"kernel_size\"와 같은 값이 사용됨.\n",
        "- padding : 입력 신호 주의 추가할 패딩의 크기를 나타냄."
      ],
      "metadata": {
        "id": "79b4MRcVE-Te"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력 텐서 생성 (배치 크기 : 1, 채널 : 1 , 시퀀스 길이 : 10)\n",
        "input_tensor = torch.randn(1, 1, 10)\n",
        "\n",
        "# MaxPool1d 인스턴스 생성\n",
        "maxpool = nn.MaxPool1d(kernel_size = 2, stride = 2, padding = 0) # kernel_size = 2로 설정하여 2개의 연속된 값 중 최대값을 선택, stride = 2로 설정하여 풀링 윈도우를 2개의 값씩 이동하며 수행.\n",
        "\n",
        "# MaxPooling 수행\n",
        "output_tensor = maxpool(input_tensor)\n",
        "\n",
        "# 입력 텐서와 출력 텐서의 크기 확인\n",
        "print(\"Input tensor size:\", input_tensor.size())\n",
        "print(\"Output tensor size:\", output_tensor.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZblM1b9DHKBT",
        "outputId": "98679211-f86f-4c0a-dab8-c5aa7e0da78f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tensor size: torch.Size([1, 1, 10])\n",
            "Output tensor size: torch.Size([1, 1, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## nn.ModuleList()\n",
        "-> PyTorch에서 사용되는 모듈들을 리스트 형태로 관리하는 클래스. **동적으로 모듈의 추가 / 삭제가** 가능해진다.\n",
        "\n",
        "- nn.Module을 상속한 클래스 내부에서 사용된다. 서브모듈들을 리스트 형태로 정의하고, 해당 리스트를 모델 클래스의 속성(self.\"attrname\")으로 사용한다. 자동으로 모델 파라미터들과 함께 관리되며, 모델의 forward 연산에서 호출될 수 있다.\n",
        "\n",
        "- nn.ModuleList 내 nn.Module 각각은, nn.Sequential을 사용할 때 처럼 모듈 간에 Connection이 존재하는 것이 아님. 파이썬에서 int, str, 등등을 하나의 리스트에 저장하는 것과 동일함.\n",
        "\n",
        "- 장점 : PyTorch가 nn.ModuleList 내부에, \"nn.Module\"들이 \"존재한다는 것을\n",
        "인지\" == \"aware\"한다. 두 종류의 module이 받는 input이 서로 다르고, 여러 개를\n",
        "반복적으로 정의해야 할 때 유용하게 사용할 수 있다.\n",
        "Module들을 Python list에 넣어 보관한다면, 꼭 마지막에\n",
        "이들을 nn.ModuleList로 wrapping 해줘야 한다."
      ],
      "metadata": {
        "id": "UohV5ajhIENo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# 여러 개의 레이어를 동적으로 관리하는 모델\n",
        "# nn.ModuleList 사용하여 nn.Linear 모듈 5개 추가\n",
        "# forward 함수에서는 nn.ModuleList에 있는 모듈들을 순차적으로 호출하여\n",
        "# forward 연산을 수행한다.\n",
        "\"\"\"\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "\n",
        "        self.linears = nn.ModuleList()\n",
        "        for i in range(5):\n",
        "            self.linears.append(nn.Linear(10, 20))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.linears:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, n_blocks):\n",
        "        super(Network, self).__init__()\n",
        "        self.n_blocks = n_blocks\n",
        "        block_A_list = []\n",
        "        block_B_list = []\n",
        "\n",
        "        for _ in range(n_blocks):\n",
        "            block_A_list.append(\n",
        "                # Block_A() -> 기 정의 가정\n",
        "            )\n",
        "            block_B_list.append(\n",
        "                # Block_B() -> 기 정의 가정\n",
        "            )\n",
        "        self.block_A_list = nn.ModuleList(block_A_list)\n",
        "        self.block_B_list = nn.ModuleList(block_B_list)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        for i in range(self.n_blocks):\n",
        "            out = self.block_A_list[i](x)\n",
        "            out = self.block_B_list[i](out, y)\n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "id": "jVS4Eg7lHo45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## nn.ReLU\n",
        "-> PyTorch에서 사용되는 ReLU activation function을 구현한 클래스. \"ReLU(x) = max(0, x)\"\n",
        "\n",
        "- nn.ReLU()에는 inplace 옵션이 존재하므로, 이를 사용하면 ReLU 함수 연산이 in-place로 수행되어 입력 텐서의 메모리를 직접적으로 수정하여 연산 속도를 향상시키는 효과가 있다.\n",
        "- inplace = False가 기본값이므로 기본적으로는 원본 텐서를 수정하지 않고 새로운 텐서를 리턴한다.\n",
        "- inplace = True로 설정 시, 입력 텐서를 직접 수정하게 된다."
      ],
      "metadata": {
        "id": "Bif2JjZf3PP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ReLU 레이어 인스턴스화\n",
        "relu = nn.ReLU() # inplace = False -> 기본값\n",
        "\n",
        "# 연산 적용\n",
        "x = torch.randn(1, 5) # torch.randn(5) != torch.randn(1, 5) (2차원)\n",
        "print(\"x :\", x)\n",
        "y = relu(x) # 원본 x가 수정되지 않고 새로운 텐서 y를 리턴함\n",
        "print(\"after ReLU()\")\n",
        "print(\"x :\", x)\n",
        "print(\"y :\", y)\n",
        "\n",
        "print()\n",
        "\n",
        "# inplace = True로 설정한 ReLU 연산\n",
        "x = torch.randn(5)\n",
        "print(\"x :\", x)\n",
        "relu_inplace = nn.ReLU(inplace = True)\n",
        "y = relu_inplace(x)\n",
        "print('after ReLU(inplace=True)')\n",
        "print('x :', x)\n",
        "print('y :', y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kwSPe9RI1vg",
        "outputId": "7834ec10-2230-40d3-edd3-4d155cccbc0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x : tensor([[ 0.4156, -0.1641, -0.5723,  0.1554,  0.4558]])\n",
            "after ReLU()\n",
            "x : tensor([[ 0.4156, -0.1641, -0.5723,  0.1554,  0.4558]])\n",
            "y : tensor([[0.4156, 0.0000, 0.0000, 0.1554, 0.4558]])\n",
            "\n",
            "x : tensor([ 0.8169,  0.8675,  0.0137, -0.4744, -0.2465])\n",
            "after ReLU(inplace=True)\n",
            "x : tensor([0.8169, 0.8675, 0.0137, 0.0000, 0.0000])\n",
            "y : tensor([0.8169, 0.8675, 0.0137, 0.0000, 0.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### nn.LeakyReLU()\n",
        "-> leaky relu 지원. 입력값이 음수일 경우 기울기를 0이 아닌 작은 값으로 유지한다. ReLU 함수에서 발생하는 \"죽은 뉴런(dead neuron)\" 문제를 완화할 수 있다.\n",
        "\n",
        "- 매개변수로 negative_slope를 받는다. 입력값이 음수일 때 사용할 기울기 값을 결정한다. 0.01을 주로 사용한다."
      ],
      "metadata": {
        "id": "HPzOGeLc5x1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 은닉층의 Activation Function으로 LeakyReLU를 사용하여 비선형성을 추가한다.\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.fc1 = nn.Linear(100, 50)\n",
        "        self.relu = nn.LeakyReLU(negative_slope = 0.01) # self.relu에 LeakyReLU 적용\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "JedacX3Y4KPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 학습\n",
        "\n",
        "-> Optimizer(옵티마이저)를 활용한 가중치 업데이트 과정을 학습한다.\n",
        "\n",
        "1. 모델 초기화(클래스 생성자로)\n",
        "2. 손실 함수 정의\n",
        "3. 손실 함수 \"옵티마이저(손실 함수의 loss 최적화)\" 정의\n",
        "4. DataLoader를 사용하여 mini-batch로 로딩\n",
        "5. 학습 루프\n",
        "6. 모델 학습 완료 후 예측 등의 후처리 작업을 진행한다.\n",
        "\n",
        "- optim.zero_grad() : 옵티마이저에 연결된 모델의 모든 가중치에 대한 그라디언트를 초기화한다. 새로운 미니배치에 대한 그라디언트 계산 전에 이전 미니배치의 그라디언트가 남아있는 것을 방지하기 위해 사용된다.\n",
        "\n",
        "- loss.backward() : 손실(loss)에 대한 backpropagation을 수행한다. 역전파는 손실을 최소화하기 위해 모델의 각 가중치에 대한 그라디언트를 계산하는 과정이다. 가중치를 업데이트하는 데 사용된다.\n",
        "\n",
        "- optim.step() : 옵티마이저를 사용하여 모델의 가중치를 업데이트한다. loss를 최소화하는 방향(그라디언트의 반대 방향)으로 모델을 학습시킨다. 옵티마이저의 설정에 따라 가중치를 업데이트하는 방식이 다양하게 사용될 수 있다. 예를 들어 경사하강법(GD 개열)을 사용하는 경우, 가중치는 학습률과 그래디언트의 곱에 따라 업데이트될 수 있다."
      ],
      "metadata": {
        "id": "oz6QOHQowZwO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 구체화 코드\n",
        "-> 모델 학습이 완료된 후, 새로운 데이터를 사용하여 예측을 수행할 수 있다.\n",
        "\n",
        "예측 작업을 위해서는 \"모델의 forward() 메소드를 호출\"하여 입력 데이터에 대한 예측값을 얻고 이를 적절히 처리하여 원하는 형태로 출력하면 된다.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "이 모델은 다중 클래스 분류 모델로, 각 입력 샘플이 5개의 클래스(0, 1, 2, 3, 4) 중 하나에 속하는지를 예측합니다. 모델은 다음과 같은 방식으로 작동합니다:\n",
        "\n",
        "입력: 10차원 벡터<br>\n",
        "출력: 5차원 벡터, 각 요소는 해당 클래스에 대한 점수 (logits)\n",
        "학습 루프에서는 각 배치의 입력 데이터를 모델에 전달하여 출력값을 얻고, 손실 함수 (여기서는 CrossEntropyLoss, logits을 Softmax에 통과시킴)를 사용하여 예측값과 실제 레이블 간의 손실을 계산합니다. 그런 다음, 역전파를 통해 손실에 대한 그라디언트를 계산하고, 옵티마이저를 사용하여 모델의 가중치를 업데이트합니다.\n",
        "\n",
        "학습이 완료된 후, 모델은 새로운 입력 데이터에 대해 예측을 수행할 수 있습니다. 예측 과정에서는 모델에 새로운 데이터를 입력으로 전달하고, 출력값에서 가장 높은 점수를 가지는 클래스를 선택하여 최종 예측 결과를 얻습니다.\n",
        "\n",
        "코드에서는 torch.argmax(predictions, dim=1)을 사용하여 각 입력 샘플에 대해 가장 높은 점수를 가지는 클래스를 예측합니다. 최종적으로 새로운 데이터에 대한 클래스 예측 결과를 출력합니다."
      ],
      "metadata": {
        "id": "1NWpddOS1EFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# 가정: 입력 데이터 X와 레이블 y\n",
        "X = torch.randn(100, 10) # 100개의 샘플, 각 샘플은 10차원 벡터\n",
        "y = torch.randint(0, 5, (100,)) # 0에서 4 사이의 정수를 가지는 100개의 레이블\n",
        "\n",
        "# TensorDataset과 DataLoader를 사용하여 데이터셋과 데이터로더 생성\n",
        "dataset = TensorDataset(X, y)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True) # 미니배치 크기를 16으로 설정\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(10, 5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "# 1. 모델 초기화\n",
        "model = MyModel()\n",
        "\n",
        "# 2. 손실 함수 정의\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# 3. Optimizer 정의\n",
        "optim = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "# model.parameters() 메소드 : PyTorch 모델의 학습 가능한 파라미터들을 리턴하는 메소드. 모든 학습 가능한 파라미터들을 Python의 generator 객체로 반환한다. 학습 중 업데이트되는 값.\n",
        "\n",
        "# 4. 학습 루프 정의\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_X, batch_y in dataloader: # DataLoader 객체 dataloader를 사용하여 미니배치 단위로 데이터 로딩\n",
        "        # 순전파\n",
        "        outputs = model(batch_X)\n",
        "\n",
        "        # 손실 계산\n",
        "        loss = loss_fn(outputs, batch_y)\n",
        "\n",
        "        # 역전파\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "    # 학습 과정 출력\n",
        "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, loss.item()))\n",
        "\n",
        "# 모델 학습 완료 후 예측 작업 수행\n",
        "model.eval()  # 모델을 평가 모드로 전환\n",
        "\n",
        "# 새로운 입력 데이터 (예를 들어, 새로운 10차원 벡터 5개)\n",
        "new_data = torch.randn(5, 10)\n",
        "\n",
        "# 예측 수행\n",
        "with torch.no_grad():  # 평가 시에는 그라디언트 계산을 하지 않음\n",
        "    predictions = model(new_data)\n",
        "    predicted_classes = torch.argmax(predictions, dim=1)  # 가장 높은 값을 가지는 클래스 선택\n",
        "\n",
        "print(\"New data predictions:\", predicted_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MU5sEtNJ0-0A",
        "outputId": "05facca9-400f-4c11-8f9e-eab0957a8408"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 2.1135\n",
            "Epoch [2/10], Loss: 1.6379\n",
            "Epoch [3/10], Loss: 2.0554\n",
            "Epoch [4/10], Loss: 2.1626\n",
            "Epoch [5/10], Loss: 1.6265\n",
            "Epoch [6/10], Loss: 2.1048\n",
            "Epoch [7/10], Loss: 1.8024\n",
            "Epoch [8/10], Loss: 1.4925\n",
            "Epoch [9/10], Loss: 2.0947\n",
            "Epoch [10/10], Loss: 1.8524\n",
            "New data predictions: tensor([3, 3, 0, 1, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## \"torch.optim\" : 다양한 최적화 알고리즘 지원한다.\n",
        "- SGD\n",
        "- Adam\n",
        "- RMSProp\n",
        "- Adagrad\n",
        "- AdamW\n",
        "\n",
        "각 최적화 알고리즘은 다양한 하이퍼파라미터를 가지고 있어, 사용자는 필요에 따라 학습률(learning rate), 가중 평균 계수(momentum, beta 등), 그라디언트 클리핑(clipping) 등을 조절하여 최적화 알고리즘의 동작을 제어할 수 있다."
      ],
      "metadata": {
        "id": "r9RSxuyv5W77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 사용 예제\n",
        "import torch.optim as optim\n",
        "\n",
        "# 모델 정의 (상기 정의한 모델 사용)\n",
        "model = MyModel()\n",
        "\n",
        "# 최적화 알고리즘 선택 및 하이퍼파라미터 할당(학습률, 모멘텀 지수)\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.9)\n",
        "\n",
        "# 학습 과정에서 가중치 업데이트\n",
        "optimizer.zero_grad() # 그라디언트 초기화\n",
        "# outputs = model(inputs)\n",
        "# loss = criterion(outputs, targets)\n",
        "# loss.backward() # 그라디언트 계산\n",
        "# optimizer.step() # 가중치 업데이트"
      ],
      "metadata": {
        "id": "rXUkNCr61eDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adagrad(Adaptive Gradient Descent)\n",
        "-> 학습률을 각 파라미터에 대해 \"독립적\"으로 조절하는 방식으로 모델을 업데이트하는 옵티마이저. 이전 그라디언트의 \"제곱의 누적 값\"을 사용하여 학습률을 조절하므로, 그라디언트가 많이 업데이트된 파라미터는 학습률이 감소하게 되고, 적게 업데이트된 파라미터는 학습률이 증가하게 된다.\n",
        "\n",
        "이를 통해 데이터셋에 따라 학습률을 자동으로 조절하여 최적의 학습 성능을 얻는다.\n",
        "\n",
        "torch.optim.Adagrad(params, lr, lr_decay, weight_decay, initial_accumulator_value, eps)\n",
        "- lr_decay : 학습률 감소율, 기본값은 0\n",
        "- weight_decay : Weight 감쇠(L2 Regularization)계수, 기본값은 0\n",
        "- initial_accumulator_value : 그라디언트 제곱의 누적값 초기화, 기본값은 0\n",
        "- eps(입실론) : 분모를 0으로 나누는 것을 방지하기 위한 작은 상수값, 기본값은 1e-10"
      ],
      "metadata": {
        "id": "XaEZ3Suc74WE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adam(Adaptive Moment estimation)\n",
        "-> 현재 가장 널리 사용되는 옵티마이저로, 이전 그라디언트의 지수적인(1차 모멘트 추정치) / 제곱에 대한(2차 모멘트 추정치) 이동 평균을 사용하여 학습률을 조절하는 방식으로 모델을 업데이트한다. GD 기반이면서도 Momentum 및 학습률 감소와 같은 개선된 기능을 추가한 최적화 알고리즘이다.\n",
        "- betas : Adam 알고리즘에서 사용되는 두 개의 모멘텀 계수(beta1, beta2)를 튜플 형태로 전달한다. 기본값은 (0.9, 0.999)이다. 각각 1차/2차 모멘트 추정치를 의미한다.\n",
        "- amsgrad : AMSGrad 알고리즘을 사용할지에 대한 여부를 결정하는 boolean 값으로, 기본값은 false이다.\n",
        "\n",
        "Adam은 학습률을 자동으로 조절하고, 모멘텀을 활용하여 이전의 경사 정보를 이용하여 더 빠르게 수렴할 수 있는 장점이 있다.\n",
        "\n",
        "### AdamW(Adam with 'W'eight decay)\n",
        "-> Adam 옵티마이저의 변형으로, 가중치 감쇠(weight decay)를 적용하는 것이 특징이다. 가중치 감쇠는 모델의 가중치를 감소시킴으로써 복잡성을 제어하며, \"오버피팅 완화\" 효과가 있다.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H8zXOuqq9nIK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RMSProp(Root Mean Square Propagation)\n",
        "-> RNN과 같이 시퀀스 데이터를 다룰 때 사용된다. 그라디언트의 크기를 지수이동평균을 사용하여 조절하며, 경사의 크기에 따라 각각의 파라미터를 업데이트.\n",
        "\n",
        "torch.optim.RSMprop(params, lr, momentum, alpha, eps, centered, weight_decay, momentum_decay)\n",
        "- alpha(float, optional, 기본값 = 0.99) : RMSProp에서 이동평균 계산 시 사용되는 계수로 경사의 크기를 조절한다."
      ],
      "metadata": {
        "id": "Jt95Y809CmCa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SGD(Stochastic Gradient Descent)\n",
        "-> 확률적 경사하강법 최적화 알고리즘. 모멘텀 기법을 옵션으로 갖는다.\n",
        "\n",
        "각 업데이트 스텝마다 무작위로 선택된 일부 샘플(mini-batch)에 대한 손실 함수의 그라디언트를 사용하여 모델을 업데이트한다. (주머니에서 공 = 샘플을 비복원추출하는 것)\n",
        "\n",
        "torch.optim.SGD(params, lr = <required>, momentum, dampening, weight_decay, nesterov)\n",
        "- dampening : 모멘텀에 적용되는 감쇠(damp) 값으로, 기본값은 0이다.\n",
        "- nesterov : 불리언 값으로, 네스테로프 모멘텀을 사용할지 여부를 결정한다.\n",
        "\n"
      ],
      "metadata": {
        "id": "BpvrBct_DqxW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# 손실 함수(Loss Function)\n",
        "-> 모델이 예측한 값과 실제 타깃(label) 사이의 차이를 측정하여 모델의 학습을 도와주는 역할을 수행한다.\n",
        "\n",
        "- torch.nn.MSELoss : 평균제곱오차를 계산하는 손실 함수로, 회귀 문제에 사용된다.\n",
        "- torch.nn.L1Loss : 평균'절대'오차. 회귀 문제에 사용됨.\n",
        "\n",
        "---\n",
        "\n",
        "- torch.nn.CrossEntropyLoss : 대중적으로 사용된다. 분류 문제에 사용된다. \"**다중** 클래스 분류\"에 사용되며 모델의 출력값과 타깃 클래스 간 차이를 계산한다. Softmax 함수를 내장하고 있어, 모델의 출력 값에 소프트맥스를 적용할 필요 없이 모델의 출력값과 레이블을 입력으로 받아 손실을 계산한다.\n",
        "- torch.nn.BCELoss, torch.nn.BCEWithLogitsLoss : \"**이진** 클래스 분류\"에 사용되며, 'logit 값'(시그모이드 적용하지 않은 날것의 상태)에 대하여 교차 엔트로피 손실을 계산한다. 아직 확률 값으로 변환되지 않은 상태에서 손실을 계산하므로, 안정적인 계산을 제공하며 수치적 불안정성을 피할 수 있다.\n",
        "- torch.nn.NLLLoss : 음의 로그 우도(Negative Log Likelihood) 손실을 계산하는 손실 함수로, 분류 문제에 사용된다. 주로 log softmax 출력을 사용하는 다중 클래스 분류 문제에 사용된다. 모델의 출력은 로그 확률로 주어져야 하며, 타깃 레이블은 정수 값으로 주어져야 한다. 모델이 출력하는 로그 확률 중 \" 실제 차겟 \" 레이블에 해당하는 값의 로그 확률을 사용하여 NLL loss가 계산된다.\n",
        "\n",
        "---\n",
        "\n",
        "- torch.nn.KLDivLoss : KL-Divergence를 계산하는 손실 함수로, \"두 개의 확률분포\"를 받아 첫 번째 확률 분포가 두 번째 확률 분포와 얼마나 다른지를 측정하는 KL-Divergence 값을 계산한다. 주로 **생성형 모델의 분포와 실제 데이터의 분포 간의 차이**를 측정하는 등의 용도나, 분포 간의 정규화를 위해 사용된다."
      ],
      "metadata": {
        "id": "SS7Es4RPF_jc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측 값과 타깃 데이터 생성\n",
        "predictions = torch.randn(10)\n",
        "targets = torch.rand(10)\n",
        "\n",
        "# 1. 평균 제곱 오차(MSE) 손실함수 사용\n",
        "mse_loss = nn.MSELoss()\n",
        "loss_mse = mse_loss(predictions, targets)\n",
        "print(\"MSE Loss:\", loss_mse.item())\n",
        "\n",
        "# 2. 평균 절대 오차(MAE) 손실함수 사용\n",
        "mae_loss = nn.L1Loss()\n",
        "loss_mae = mae_loss(predictions, targets)\n",
        "print(\"MAE Loss:\", loss_mae.item())\n",
        "\n",
        "# 3. 교차 엔트로피 손실\n",
        "cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "loss_cross_entropy = cross_entropy_loss(predictions, targets)\n",
        "print(\"Cross Entropy Loss:\", loss_cross_entropy.item())\n",
        "\n",
        "# 4. 이진 교차 엔트로피 손실\n",
        "binary_cross_entropy_loss = nn.BCELoss()\n",
        "sigmoid = nn.Sigmoid()\n",
        "predictions_sigmoid = sigmoid(predictions) # 예측 값에 시그모이드 함수 적용, 타깃 값에는 적용하지 않는다.\n",
        "loss_binary_cross_entropy = binary_cross_entropy_loss(predictions_sigmoid, targets.float())\n",
        "print(\"Binary Cross Entropy Loss:\", loss_binary_cross_entropy.item())\n",
        "\n",
        "# 4-1. 이진 교차 엔트로피 손실(시그모이드 함수를 적용하지 않은 경우)\n",
        "binary_cross_entropy_with_logits_loss = nn.BCEWithLogitsLoss()\n",
        "loss_binary_cross_entropy_with_logits = binary_cross_entropy_with_logits_loss(predictions, targets.float())\n",
        "print(\"Binary Cross Entropy with Logits Loss:\", loss_binary_cross_entropy_with_logits.item())\n",
        "\n",
        "print('-' * 60)\n",
        "\n",
        "# 확률분포 형태의 예측값과 타깃값\n",
        "predictions = torch.tensor([[0.5, 0.2, 0.3], [0.1, 0.6, 0.3], [0.2, 0.2, 0.6]]) # 3 x 3(세 개 원소값들의 합은 1이 됨. = 확률의 의미.)\n",
        "targets = torch.tensor([0, 1, 2]) # 클래스\n",
        "\n",
        "# 5. NLL 손실\n",
        "nll_loss = nn.NLLLoss()\n",
        "log_softmax = nn.LogSoftmax(dim = 1) # '로그' 소프트맥스 함수 정의(nn.LogSoftmax 적용)\n",
        "predictions_log_softmax = log_softmax(predictions) # '로그' 소프트맥스 함수 적용\n",
        "loss_nll = nll_loss(predictions_log_softmax, targets)\n",
        "print(\"NLL Loss:\", loss_nll.item())\n",
        "\n",
        "# 6. KL-Divergence 손실\n",
        "kld_loss = nn.KLDivLoss()\n",
        "log_softmax = nn.LogSoftmax(dim = 1)\n",
        "predictions_log_softmax = log_softmax(predictions)\n",
        "\n",
        "import torch.nn.functional as F\n",
        "targets_one_hot = F.one_hot(targets, num_classes = predictions.shape[1]).float() # (중요) 타깃 값을 확률분포 형태로 변환\n",
        "loss_kld = kld_loss(predictions_log_softmax, targets_one_hot)\n",
        "print(\"KLD Loss:\", loss_kld.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0bQCs9SM5rJ",
        "outputId": "0100731b-4253-42f5-ca22-8736820edd28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE Loss: 1.0661760568618774\n",
            "MAE Loss: 0.8275183439254761\n",
            "Cross Entropy Loss: 13.998072624206543\n",
            "Binary Cross Entropy Loss: 0.8257573246955872\n",
            "Binary Cross Entropy with Logits Loss: 0.8257573246955872\n",
            "------------------------------------------------------------\n",
            "NLL Loss: 0.8811807036399841\n",
            "KLD Loss: 0.2937268912792206\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2976: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### model.train() : \"모델 학습 모드 설정 메소드\"\n",
        "-> 보통 모델을 학습하기 전에 호출되며, 모델이 훈련 데이터(training set)에 대해 학습을 시작할 준비를 하는 데 사용된다. 모델을 학습하기 위해 호출되는 함수로, 파라미터 업데이트 및 그라디언트 계산을 가능케 해준다.\n",
        "\n",
        "<br>\n",
        "\n",
        "- 드롭아웃(Dropout) 및 Batch Normalization과 같은 기법들을 동작하도록 한다. 이는 모델이 데이터에 대하여 일반화되는 것을 돕는다.\n",
        "- 그라디언트 계산을 가능케 한다. train 모드에서는 모델의 파라미터 업데이트를 위해 역전파(backpropagation)을 수행하고, 그라디언트를 계산한다.\n",
        "- 모델의 파라미터를, Optimizer 알고리즘에 따라 업데이트가 가능하도록 한다.\n",
        "\n"
      ],
      "metadata": {
        "id": "TokhVr3kHzM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        # 모델의 레이어들을 정의한다.\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 모델의 순전파 연산을 정의한다.\n",
        "        pass\n",
        "\n",
        "# 모델 인스턴스 생성\n",
        "model = MyModel()\n",
        "\n",
        "# 모델을 \"학습 모드\"로 설정\n",
        "model.train()\n",
        "\n",
        "# 추후(생략) : 모델을 사용하여 학습 데이터에 대해 forward 연산 수행 -> backpropagation 수행하여 W, B 학습(그라디언트 계산 및 파라미터 업데이트 수행.)"
      ],
      "metadata": {
        "id": "tYEPIIkEQ3qs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "567570e0-8b4d-480a-dd07-e2ae427b0f5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyModel()"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 추론\n",
        "\n",
        "## 1) model.eval() : PyTorch에서 모델을 평가 모드로 전환하는 메소드이다. 별도의 인수를 받지 않으며, 단순히 model.eval()을 호출하면 모델이 평가 모드로 전환된다.\n",
        "\n",
        "- Training 시 필요했던 Dropout 및 Batch Normalization 기능이 \"비활성화\"된다. 배치 정규화의 이동평균/분산이 업데이트되지 않는다.\n",
        "\n",
        "- 주로 테스트(test, 실제 예측) 데이터나 검증(validation) 데이터를 사용하여 모델을 평가할 때 사용된다.\n",
        "\n",
        "- 평가 모드에서는 모델이 \"추론 시에 동일한 동작\"을 수행하도록 설정되어 있어, 모델의 성능 평가에 불필요한 노이즈를 줄이고 일관된 결과를 얻을 수 있다.\n",
        "\n",
        "-> 모델을 학습(training)하는 동안 사용한 모델 객체를 추론(inference)할 때 model.eval()을 호출하여 추론 모드로 전환하고, \"추론이 끝난 후에는 다시 model.train()\"을 호출하여 학습 모드로 전환하는 것이 일반적이다.\n",
        "\n",
        "<br>\n",
        "\n",
        "## 2) with torch.inference_mode() : # PyTorch에서 제공하는 컨텍스트 매니저로, 추론 과정에서 \"모델의 성능을 최적화\"하기 위해 사용되는 모드이다. 추론 모드에 돌입하면, 학습 시에 사용되는 일부 기능들을 자동으로 비활성화하여 모델의 실행 속도 및 효율성을 향상시킬 수 있다.\n",
        "\n",
        "- 별도의 인수를 받지 않고, \"with\"문 안에 들어가는 것만으로 모델을 추론 모드로 전환한다. with 문을 빠져나가면 모델은 다시 기존의 모드(학습/평가모드)로 돌아간다.\n",
        "\n",
        "- 드롭아웃, 배치 정규화 비활성화 등의 추론 관련 동작을 수행한다.\n",
        "\n",
        "> 사용 효과\n",
        "\n",
        "1. Autograd 비활성화 : 추론 시에는 모델의 그라디언트 계산이 불필요하기 떄문. 연산속도 향상 가능\n",
        "2. 메모리 최적화 : 추론 시 중간 결과값을 유지할 필요가 없기 때문에, 값을 메모리에 저장하지 않고 바로 해제(deallocation)하여 메모리 사용량을 줄일 수 있다.\n",
        "3. 장치 복사 최적화 : 추론 시에는 모델의 입력과 출력을 복사하는 불필요한 연산을 제거하여 추론 속도를 향상시킬 수 있다.\n",
        "\n",
        "<br>\n",
        "\n",
        "## 3) ReduceLROnPlateau : (LR : Learning Rate 의미, Plateau : 고원) loss function이 평평한 고원과 같은 개형일 때 모델의 성능이 더 이상 향상되지 않고 정체되는데, 학습률을 감소시킴으로써 손실 함수의 평평한 구역을 벗어나 더 나은 최소점(better minima)을 찾도록 돕는다는 의미에서 유래되었다.\n",
        "\n",
        "- Validation Loss가 더 이상 개선되지 않는 경우 학습률을 동적으로 감소시켜 모델의 학습을 돕는 기법이다. 학습 중 주기적으로 검증 데이터셋의 손실을 모니터링하고, 기 정의된 조건에 따라 학습률을 감소시킨다. 매개변수를 적절히 활용하여 Early Stopping을 구현하고, 검증 데이터셋의 성능이 개선되지 않을 때 learning rate를 동적으로 조절하여 학습을 최적화할 수 있다.\n",
        "\n",
        "- torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode, factor, patience, threshold, threshold_mode, cooldown, min_lr, eps, verbose) # verbose는 더 이상 사용하지 않는다.\n",
        "\n",
        "> scheduler.num_bad_epochs(Variable) : ReduceLROnPlateau 스케줄러와 함께 사용되며, 학습 중 학습률을 동적으로 조절하는 데 사용된다. 스케줄러 사용 시, \"현재 학습률이 개선되지 않은 연속적인 Epoch의 개수\"를 나타내는 변수이다. <br><br> 이 변수는 ReduceLROnPlateau 스케줄러에서 정의된 patience 매개변수와 관련이 있다. patience는 개선이 멈춘 후에 추가적인 Epoch 횟수를 지정하는데, num_bad_epochs가 patience보다 크거나 같아지면 학습률이 조정된다. <br> 예시 : patience가 3인 경우, 만약 연속적으로 3개의 에포크 동안 검증 손실이 개선되지 않는다면 (num_bad_epochs >= patience), 스케줄러는 학습률을 조정할 것이다. <br> 개선이 이루어지면 num_bad_epochs가 0으로 초기화된다. 이를 통해 스케줄러가 일정 조건을 만족하지 않는 연속적인 에포크들을 추적하고, 일정 횟수 이상의 연속적인 개선이 없을 경우 학습률을 조정하는 기능을 수행할 수 있다."
      ],
      "metadata": {
        "id": "abfbWg9X8KyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "    PyTorch에서 Early Stopping, 모델 저장 및 로딩, ReduceLROnPlateau를 한 번에 구현한 예제 소스코드 (sklearn 라이브러리 또한 사용함)\n",
        "\n",
        "    <설명>\n",
        "    - MyModel : 가상의 모델 클래스. 3개의 FC Layer로 구성됨\n",
        "\n",
        "    - 데이터 생성 및 데이터셋 분리 : 가상의 데이터 생성, 학습 / 검증 데이터로 분리 진행.\n",
        "\n",
        "    - 모델 초기화 : 인스턴스(model) 생성, loss function(크로스 앤트로피)과 optimizing algorithm(아담) 정의\n",
        "\n",
        "    - Early Stopping 관련 변수 초기화 : Early Stopping을 위한 변수들을 초기화. 가장 낮은 Validation Loss 값을 저장하고,\n",
        "      개선이 없는 Epoch의 수를 세기 위한 변수들인 patience와 no_improvement들이 존재.\n",
        "\n",
        "    - ReduceLROnPlateau 관련 변수 초기화: 학습률 조절을 위한 ReduceLROnPlateau 스케줄러를 초기화함.\n",
        "\n",
        "    - 모델 학습 루프 : 학습 데이터로 forward, backward, optimizer step을 수행하고, 검증 데이터로 검증 손실을 계산\n",
        "      검증 손실을 이용하여 학습률을 조절하고 \"검증 손실이 이전 최적 손실보다 낮으면 모델을 저장\"\n",
        "      개선이 없는 경우 Early stopping 수행.\n",
        "\n",
        "    - main() : Early stopping 시점에서 저장한 최적 모델 로딩, 이를 이용하여 테스트 데이터에 대한 예측 수행.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn ## nn.Module 및 기타\n",
        "import torch.optim as optim ## Optimizer 사용\n",
        "\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split # Train 데이터 / Validation 데이터 분리 위한 사이킷런 라이브러리\n",
        "from sklearn.metrics import accuracy_score # 유사도 점수 평가 위한 사이킷런 라이브러리\n",
        "\n",
        "# 가상의 모델 클래스 정의\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(10, 20)\n",
        "        self.fc2 = nn.Linear(20, 10)\n",
        "        self.fc3 = nn.Linear(10, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x) # 마지막 레이어는 그냥 FC Layer. ReLU 적용 안 함\n",
        "        return x\n",
        "\n",
        "# 가상의 데이터 생성\n",
        "X = torch.randn(100, 10)\n",
        "y = torch.randint(0, 2, (100,))\n",
        "\n",
        "# sklearn 라이브러리를 이용하여 데이터셋을 학습 / 검증 데이터로 분리\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 1234)  # random_state 값을 변경하여 다양한 학습 결과 확인 가능\n",
        "\"\"\"\n",
        "    test_size : 테스트 데이터셋의 비율(float)이나 개수(int) (default = 0.25),\n",
        "    train_size : 학습 데이터셋의 비율(float)이나 개수(int) (default = test_size의 나머지),\n",
        "    random_state : 데이터 분할시 셔플이 이루어지는데, 이를 위한 시드값(int나 RandomState로 입력)\n",
        "\"\"\"\n",
        "\n",
        "# 모델 초기화\n",
        "model = MyModel()\n",
        "criterion = nn.CrossEntropyLoss() # for Classification\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.01) # 옵티마이저 : 범용적인 Adam 사용\n",
        "\n",
        "# (중요) Early Stopping 관련 변수 초기화\n",
        "best_val_loss = float('inf')\n",
        "patience = 3\n",
        "num_epochs = 100\n",
        "no_improvement = 0\n",
        "\n",
        "# (중요) ReduceLROnPlateau 관련 변수 초기화\n",
        "lr_scheduler = ReduceLROnPlateau(optimizer, mode = 'min', factor = 0.1, patience = 2)\n",
        "\n",
        "\n",
        "\n",
        "### 모델 학습 루프 ###\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"epoch : {epoch}/100\")\n",
        "    model.train() # 학습 모드로 전환\n",
        "    optimizer.zero_grad() # 그라디언트 초기화\n",
        "\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train) # loss update\n",
        "    loss.backward() # backwardpropagation 진행\n",
        "    optimizer.step()\n",
        "\n",
        "    # 모델 검증(evaluation) : Training Loop 내부에서 수행함에 유의\n",
        "    model.eval() # 평가 모드로 전환\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(X_val)\n",
        "        val_loss = criterion(val_outputs, y_val)\n",
        "\n",
        "    # Early Stopping 여부 체크\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss # 최적의 Validation Loss 업데이트\n",
        "        no_improvement = 0 # 0으로 초기화(개선이 이뤄졌으므로)\n",
        "        print(\"Save best Weight and Bias\")\n",
        "        torch.save(model.state_dict(), 'best_model.pth') # 모델 저장!\n",
        "    else:\n",
        "        no_improvement += 1 # 개선이 이뤄지지 않았으므로 +1 해준다.\n",
        "\n",
        "    # Early Stopping 시행\n",
        "    if no_improvement >=  patience:\n",
        "        print(f'Early stopping after {epoch+1} epochs.')\n",
        "        break\n",
        "\n",
        "    # Learning Rate 스케줄링(조절)\n",
        "    lr_scheduler.step(val_loss)\n",
        "\n",
        "\n",
        "def main():\n",
        "    # 저장된 모델 로딩\n",
        "    best_model = MyModel()\n",
        "    best_model.load_state_dict(torch.load('best_model.pth'))\n",
        "    best_model.eval()\n",
        "\n",
        "    # 모델 추론(Test Data 이용)\n",
        "    X_test = torch.randn(10, 10) # 임의 생성한 테스트 데이터\n",
        "    with torch.no_grad():\n",
        "        test_outputs = best_model(X_test)\n",
        "        _, predicted = torch.max(test_outputs, 1)\n",
        "        predicted = predicted.numpy()\n",
        "        print(f'Predicted labels: {predicted}')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zheCVbT6qWBW",
        "outputId": "57db9db3-3cf9-4121-8786-c206f3861788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 0/100\n",
            "Save best Weight and Bias\n",
            "epoch : 1/100\n",
            "Save best Weight and Bias\n",
            "epoch : 2/100\n",
            "Save best Weight and Bias\n",
            "epoch : 3/100\n",
            "Save best Weight and Bias\n",
            "epoch : 4/100\n",
            "Save best Weight and Bias\n",
            "epoch : 5/100\n",
            "epoch : 6/100\n",
            "epoch : 7/100\n",
            "Early stopping after 8 epochs.\n",
            "Predicted labels: [0 1 0 0 0 1 1 1 1 1]\n"
          ]
        }
      ]
    }
  ]
}